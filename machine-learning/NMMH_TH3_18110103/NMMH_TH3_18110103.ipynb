{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMMH_TH3_18110103.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEvdhTT3FmV"
      },
      "source": [
        "## Machine Learning - Lab 03 - Logistic Regression \n",
        "* Full name: Đinh Anh Huy\n",
        "* Student ID: 18110103"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTEcbx-b0uVF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTtlTPEq3ObO"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp((-1)*x))\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x),axis=1).reshape(-1,1)\n",
        "\n",
        "class Logistic_Regression:\n",
        "  def __init__(self, learning_rate=0.01, max_iters=1000, random_state=0, batch_size=None, activation_function = None):\n",
        "    self.W = None\n",
        "    self.batch_size = batch_size\n",
        "    self.random_state = random_state\n",
        "    self.lr = learning_rate\n",
        "    self.max_iters = max_iters\n",
        "    self.loss_values = []\n",
        "    self.weights = []\n",
        "    self.function = activation_function\n",
        "\n",
        "  def init_W(self, X, y):\n",
        "    np.random.seed(self.random_state)\n",
        "    no_labels = np.unique(y).size\n",
        "    if no_labels <= 2:\n",
        "      self.W = np.random.randn(X.shape[1]+1, 1)\n",
        "    else:\n",
        "      self.W = np.random.randn(X.shape[1]+1, no_labels)\n",
        "  \n",
        "  def init_function(self, y):\n",
        "    if self.function == None: \n",
        "      if np.unique(y).size == 2:\n",
        "        self.function = \"sigmoid\"\n",
        "      else:\n",
        "        self.function = \"softmax\"\n",
        "\n",
        "  def gradient(self, X, y, prediction):\n",
        "    if self.function == \"sigmoid\":\n",
        "      return np.mean((prediction - y)*X, axis=0).reshape(-1,1)\n",
        "    else:\n",
        "      a = prediction - y\n",
        "      gradient = np.zeros([X.shape[1],y.shape[1]])\n",
        "      for i in range(y.shape[1]):\n",
        "        gradient[:,i:i+1] = np.mean(a[:,i:i+1]*X, axis=0).reshape(-1,1)\n",
        "      return gradient\n",
        "\n",
        "  def updateWeights(self, theta_old, gradient, learning_rate):\n",
        "    return theta_old - learning_rate*gradient\n",
        "\n",
        "  def computeLoss(self, X, y, prediction):\n",
        "    if self.function == \"sigmoid\":\n",
        "      return -np.mean((y*np.log(prediction) + (1-y)*np.log(1-prediction)), axis=0)\n",
        "    else:\n",
        "      for i in range(y.shape[1]):\n",
        "        error = (-1*y) * np.log(prediction)\n",
        "        error = np.sum(np.mean(error, axis=0))\n",
        "      return error\n",
        "\n",
        "  def prediction(self, X):\n",
        "    Z = X.dot(self.W)\n",
        "    if self.function == \"sigmoid\":\n",
        "      return sigmoid(Z)\n",
        "    return softmax(Z)\n",
        "\n",
        "  def BGD(self, X, y):\n",
        "    for epoch in range(self.max_iters):\n",
        "      pred = self.prediction(X)\n",
        "      grad = self.gradient(X, y, pred)\n",
        "      self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, pred))\n",
        "\n",
        "  def SGD(self, X, y):\n",
        "    for epoch in range(self.max_iters//X.shape[0]):\n",
        "      rd_id = np.random.permutation(X.shape[0])\n",
        "      for i in rd_id:\n",
        "        X_batch = X[j,:].reshape(1,-1)\n",
        "        y_batch = y[j,:].reshape(1,-1)\n",
        "        pred = self.prediction(X_batch)\n",
        "        grad = self.gradient(X_batch, y_batch, pred)\n",
        "        self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, self.prediction(X)))        \n",
        "  \n",
        "  def Mini_BGD(self, X, y, batch_size):\n",
        "    first_X = X.copy()\n",
        "    first_y = y.copy()\n",
        "    for epoch in range(self.max_iters//(X.shape[0]//self.batch_size + 1)):\n",
        "      indices = np.random.permutation(X.shape[0])\n",
        "      first_X = first_X[indices]\n",
        "      first_y = first_y[indices]\n",
        "\n",
        "      temp = batch_size\n",
        "      for j in range(0, X.shape[0], batch_size):    \n",
        "        X_batch = first_X[j:temp, :]\n",
        "        y_batch = first_y[j:temp,:]\n",
        "        pred = self.prediction(X_batch)\n",
        "        grad = self.gradient(X_batch, y_batch, pred)\n",
        "        self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "\n",
        "        temp = temp + batch_size if X.shape[0]-temp >= batch_size else X.shape[0]\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, self.prediction(X)))\n",
        "    \n",
        "  def fit(self, X, y):\n",
        "    self.loss_values.clear()\n",
        "    self.weights.clear()\n",
        "    self.init_W(X, y)\n",
        "    self.init_function(y)\n",
        "\n",
        "    X1 = np.concatenate([np.ones([X.shape[0],1]), X], axis=1)\n",
        "    if self.function == \"softmax\":\n",
        "      y = onehot_encoder(y)\n",
        "    else:\n",
        "      y = y.reshape(-1, 1)\n",
        "    \n",
        "\n",
        "    if (self.batch_size == None) or (self.batch_size == X.shape[0]):\n",
        "      self.BGD(X1, y)\n",
        "    elif (self.batch_size > 1) and (self.batch_size < X.shape[0]):\n",
        "      self.Mini_BGD(X1, y, self.batch_size)\n",
        "    elif self.batch_size == 1:\n",
        "      self.SGD(X1, y)\n",
        "    else:\n",
        "      raise Exception(\"Batch size is impossible.\")\n",
        "\n",
        "  def predict(self, X):\n",
        "    X1 = np.concatenate([np.ones([X.shape[0],1]), X], axis=1)\n",
        "    Z = X1.dot(self.W)\n",
        "    if self.function == \"sigmoid\":\n",
        "      prediction = sigmoid(Z)\n",
        "      return np.where(prediction>0.5,1,0).reshape(1,-1)\n",
        "    else:\n",
        "      prediction = softmax(Z)\n",
        "      return np.argmax(prediction,axis=1)\n",
        "      \n",
        "\n",
        "  def coef_(self):\n",
        "    return self.W[1:,:]\n",
        "\n",
        "  def intercept_(self):\n",
        "    return self.W[1,:]\n",
        "\n",
        "  def score(self, X, y):\n",
        "    if self.function == \"sigmoid\":\n",
        "      y = y.reshape(1,-1).T\n",
        "    return np.mean(y==self.predict(X).T)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwIGBOPLq_Pi"
      },
      "source": [
        "def scaler(X_train, X_test):\n",
        "  max = np.max(X_train,axis=0)\n",
        "  min = np.min(X_train,axis=0)\n",
        "\n",
        "  X_train = (X_train-min)/(max-min)\n",
        "  X_test = (X_test-min)/(max-min)\n",
        "  return X_train, X_test\n",
        "\n",
        "def onehot_encoder(y):\n",
        "  y_onehot = np.zeros( (y.size, y.max() + 1), dtype=int)\n",
        "  y_onehot[np.arange(y.size), y.reshape(-1)] = 1\n",
        "  return y_onehot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfqCfZxBlH8X"
      },
      "source": [
        "### Bài tập 1. Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "-bwfGfQIk7UY",
        "outputId": "77700d22-8fc4-473b-9579-2458d9c8b9bb"
      },
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-03/heart.csv\")\n",
        "data.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>148</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "      <td>294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>172</td>\n",
              "      <td>199</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "5   57    1   0       140   192    0  ...      0      0.4      1   0     1       1\n",
              "6   56    0   1       140   294    0  ...      0      1.3      1   0     2       1\n",
              "7   44    1   1       120   263    0  ...      0      0.0      2   0     3       1\n",
              "8   52    1   2       172   199    1  ...      0      0.5      2   0     3       1\n",
              "9   57    1   2       150   168    0  ...      0      1.6      2   0     2       1\n",
              "\n",
              "[10 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTagCWN-m_qj"
      },
      "source": [
        "# Split data, 33% for test and 67% for train\n",
        "X = data.drop(['target'], axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.33, random_state=42)\n",
        "\n",
        "# Scale data using min-max method\n",
        "X_train, X_test = scaler(X_train, X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bZs66Y1nNzY",
        "outputId": "3a99182e-510d-49c7-801a-b0e18e3c937c"
      },
      "source": [
        "lr = Logistic_Regression(learning_rate=0.1, max_iters=30000, random_state=10)\n",
        "lr.fit(X_train, y_train)\n",
        "for i in range(0, len(lr.loss_values), 1000):\n",
        "  print(\"Loss at iter {}: {}\".format(i, lr.loss_values[i]))\n",
        "print(\"\\n\", \"-\"*40, \"\\n\")\n",
        "print(\">> Final loss: \", lr.loss_values[-1])\n",
        "print(\">> Final W: \\n\", lr.W)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at iter 0: [0.99859025]\n",
            "Loss at iter 1000: [0.34206429]\n",
            "Loss at iter 2000: [0.32313667]\n",
            "Loss at iter 3000: [0.31577836]\n",
            "Loss at iter 4000: [0.31207608]\n",
            "Loss at iter 5000: [0.31000355]\n",
            "Loss at iter 6000: [0.30877428]\n",
            "Loss at iter 7000: [0.30801748]\n",
            "Loss at iter 8000: [0.30753881]\n",
            "Loss at iter 9000: [0.30722959]\n",
            "Loss at iter 10000: [0.3070263]\n",
            "Loss at iter 11000: [0.30689058]\n",
            "Loss at iter 12000: [0.30679871]\n",
            "Loss at iter 13000: [0.30673572]\n",
            "Loss at iter 14000: [0.306692]\n",
            "Loss at iter 15000: [0.30666127]\n",
            "Loss at iter 16000: [0.30663943]\n",
            "Loss at iter 17000: [0.30662371]\n",
            "Loss at iter 18000: [0.30661226]\n",
            "Loss at iter 19000: [0.30660382]\n",
            "Loss at iter 20000: [0.30659753]\n",
            "Loss at iter 21000: [0.30659278]\n",
            "Loss at iter 22000: [0.30658917]\n",
            "Loss at iter 23000: [0.30658638]\n",
            "Loss at iter 24000: [0.30658421]\n",
            "Loss at iter 25000: [0.30658251]\n",
            "Loss at iter 26000: [0.30658116]\n",
            "Loss at iter 27000: [0.30658008]\n",
            "Loss at iter 28000: [0.30657922]\n",
            "Loss at iter 29000: [0.30657852]\n",
            "\n",
            " ---------------------------------------- \n",
            "\n",
            ">> Final loss:  [0.30657795]\n",
            ">> Final W: \n",
            " [[ 2.19167282]\n",
            " [ 1.00994856]\n",
            " [-1.45316221]\n",
            " [ 3.00421922]\n",
            " [-0.96587053]\n",
            " [-0.82193036]\n",
            " [ 0.32264186]\n",
            " [ 1.31644471]\n",
            " [ 1.92340993]\n",
            " [-1.16824963]\n",
            " [-2.52225591]\n",
            " [ 2.12561531]\n",
            " [-5.00528297]\n",
            " [-4.28452415]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "byQlAlH0oyYD",
        "outputId": "f8abbc6b-b2b0-426f-bffe-f0a758a321e6"
      },
      "source": [
        "# Plot loss versus interations\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(lr.loss_values)\n",
        "ax.set_title('Loss versus iterations')\n",
        "ax.set(xlabel='iterations', ylabel='loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80JktnBPYYwF",
        "outputId": "78cbf952-4ec4-4cf0-e8d9-67afc07079cf"
      },
      "source": [
        "print(\">> Original label: \\n\", y_test)\n",
        "y_pred = lr.predict(X_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred.flatten())\n",
        "print(\"\\n>> Score: \", lr.score(X_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "\n",
            ">> Predicted label: \n",
            " [0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0]\n",
            "\n",
            ">> Score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXNiSmw1MUQW",
        "outputId": "cec56388-40f5-48d0-cb79-ffdf1c1a997b"
      },
      "source": [
        "print(\">> Accuracy score: \", accuracy_score(y_test, y_pred.flatten()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Accuracy score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35R_63EapJDF"
      },
      "source": [
        "**Kết quả bài toán Logistic Regression với bộ dữ liệu *Heart* chạy bằng thư viện scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRjSP4T40ZFn",
        "outputId": "5e3cad99-577d-4fe6-a3ab-1e5e4f425c9a"
      },
      "source": [
        "lr_sklearn = LogisticRegression(random_state=10)\n",
        "lr_sklearn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sklearn = lr_sklearn.predict(X_test)\n",
        "score_sklearn = lr_sklearn.score(X_test, y_test)\n",
        "\n",
        "print(\">> Original label: \\n\", y_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred_sklearn.flatten())\n",
        "print(\"\\n>> Score: \", score_sklearn)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "\n",
            ">> Predicted label: \n",
            " [0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0]\n",
            "\n",
            ">> Score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCYj3AQKLi66",
        "outputId": "9a06d9d8-d51d-4b06-9873-52d3060225d3"
      },
      "source": [
        "print(\">> Accuracy score: \", accuracy_score(y_test, y_pred_sklearn))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Accuracy score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2miKilgK5-Kg"
      },
      "source": [
        "### Bài tập 2. Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFNlJb7D6Sm8"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "y = iris_data.target\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.33, random_state=57)\n",
        "X_train2, X_test2 = scaler(X_train2, X_test2)\n",
        "\n",
        "y_test_enc = onehot_encoder(y_test2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apURiLHMpqQV",
        "outputId": "345cd69c-fa6d-47f1-ace0-bf7c895ae617"
      },
      "source": [
        "lr2 = Logistic_Regression(learning_rate=0.01, max_iters=50000, random_state=0)\n",
        "lr2.fit(X_train2, y_train2)\n",
        "for i in range(0, len(lr2.loss_values), 2000):\n",
        "  print(\"Loss at iter {}: {}\".format(i, lr2.loss_values[i]))\n",
        "print(\"\\n\", \"-\"*40, \"\\n\")\n",
        "print(\">> Final loss: \", lr2.loss_values[-1])\n",
        "print(\">> Final W: \\n\", lr2.W)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at iter 0: 1.9360183233931814\n",
            "Loss at iter 2000: 0.6313735604835801\n",
            "Loss at iter 4000: 0.5058452273146778\n",
            "Loss at iter 6000: 0.43961337025442104\n",
            "Loss at iter 8000: 0.3965167423272834\n",
            "Loss at iter 10000: 0.3653566373455009\n",
            "Loss at iter 12000: 0.341316064203433\n",
            "Loss at iter 14000: 0.3219258931905534\n",
            "Loss at iter 16000: 0.30577505777449804\n",
            "Loss at iter 18000: 0.29199347744864257\n",
            "Loss at iter 20000: 0.28001285438184487\n",
            "Loss at iter 22000: 0.2694445511483064\n",
            "Loss at iter 24000: 0.2600125240292316\n",
            "Loss at iter 26000: 0.25151432985447264\n",
            "Loss at iter 28000: 0.24379737828480139\n",
            "Loss at iter 30000: 0.23674389576466337\n",
            "Loss at iter 32000: 0.23026108332878803\n",
            "Loss at iter 34000: 0.22427448491044266\n",
            "Loss at iter 36000: 0.218723403289788\n",
            "Loss at iter 38000: 0.2135576584238891\n",
            "Loss at iter 40000: 0.20873524756489212\n",
            "Loss at iter 42000: 0.20422062459711524\n",
            "Loss at iter 44000: 0.1999834130698303\n",
            "Loss at iter 46000: 0.1959974285174081\n",
            "Loss at iter 48000: 0.19223992502560122\n",
            "\n",
            " ---------------------------------------- \n",
            "\n",
            ">> Final loss:  0.18869273222986915\n",
            ">> Final W: \n",
            " [[ 4.29673801  3.71487887 -4.86866935]\n",
            " [-0.49836716  1.93737415  1.69216632]\n",
            " [ 5.03214051 -1.78656939 -2.55005875]\n",
            " [-5.26186926  1.25865496  6.01212988]\n",
            " [-4.75749222 -1.33559133  7.41965953]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zlkm9eF6GbEL",
        "outputId": "edee46f9-a3c3-41c2-8bda-3fe3ec9d9a38"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(lr2.loss_values)\n",
        "ax.set_title('Loss versus iterations')\n",
        "ax.set(xlabel='iterations', ylabel='loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR5OPChWS6Jq",
        "outputId": "fa497bcb-777e-4c42-f2a3-320e6c929ca2"
      },
      "source": [
        "print(\">> Original label: \\n\", y_test2)\n",
        "y_pred2 = lr2.predict(X_test2)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred2)\n",
        "print(\"\\n>> Score: \", lr2.score(X_test2, y_test2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [2 2 1 1 0 1 0 2 1 0 1 0 0 0 1 2 1 0 0 2 2 0 2 2 2 0 2 2 2 2 0 2 0 0 1 2 0\n",
            " 1 1 2 1 0 2 0 0 0 1 0 2 2]\n",
            "\n",
            ">> Predicted label: \n",
            " [2 1 1 1 0 1 0 2 1 0 1 0 0 0 1 2 1 0 0 2 2 0 2 2 2 0 2 2 2 2 0 2 0 0 1 2 0\n",
            " 1 1 2 1 0 2 0 0 0 1 0 2 2]\n",
            "\n",
            ">> Score:  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2guitpc_Nwjp",
        "outputId": "2708ed51-a9cb-4972-b6cf-525d2d5f75fc"
      },
      "source": [
        "print(\">> Accuracy score: \", accuracy_score(y_test2, y_pred2.flatten()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Accuracy score:  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QBi9Wwap7Fy"
      },
      "source": [
        "**Kết quả bài toán Logistic Regression với bộ dữ liệu *Iris* chạy bằng thư viện scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCVEX-XxaqKg",
        "outputId": "cb0bb3e1-cddb-4a4d-f441-5da17e7b4844"
      },
      "source": [
        "lr2_sklearn = LogisticRegression()\n",
        "lr2_sklearn.fit(X_train2, y_train2)\n",
        "\n",
        "y_pred2_sklearn = lr2_sklearn.predict(X_test2)\n",
        "score2_sklearn = lr2_sklearn.score(X_test2, y_test2)\n",
        "\n",
        "print(\">> Original label: \\n\", y_test2)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred2_sklearn)\n",
        "print(\"\\n>> Score: \", score2_sklearn)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [2 2 1 1 0 1 0 2 1 0 1 0 0 0 1 2 1 0 0 2 2 0 2 2 2 0 2 2 2 2 0 2 0 0 1 2 0\n",
            " 1 1 2 1 0 2 0 0 0 1 0 2 2]\n",
            "\n",
            ">> Predicted label: \n",
            " [2 1 1 1 0 1 0 2 1 0 1 0 0 0 1 2 1 0 0 2 2 0 2 2 2 0 2 2 2 2 0 2 0 0 1 2 0\n",
            " 1 1 2 1 0 2 0 0 0 1 0 2 2]\n",
            "\n",
            ">> Score:  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQQ-MD9kOaU9",
        "outputId": "a6426e78-39e6-4b08-8e80-a4202be62a54"
      },
      "source": [
        "print(\">> Accuracy score: \", accuracy_score(y_test2, y_pred2_sklearn))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Accuracy score:  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}