{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMMH_TH3_18110103.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEEvdhTT3FmV"
      },
      "source": [
        "## Machine Learning - Lab 03 - Logistic Regression \n",
        "* Full name: Đinh Anh Huy\n",
        "* Student ID: 18110103"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTEcbx-b0uVF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTtlTPEq3ObO"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp((-1)*x))\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x),axis=1).reshape(-1,1)\n",
        "\n",
        "class Logistic_Regression:\n",
        "  def __init__(self, learning_rate=0.01, max_iters=1000, random_state=0, batch_size=None, activation_function = None):\n",
        "    self.W = None\n",
        "    self.batch_size = batch_size\n",
        "    self.random_state = random_state\n",
        "    self.lr = learning_rate\n",
        "    self.max_iters = max_iters\n",
        "    self.loss_values = []\n",
        "    self.weights = []\n",
        "    self.function = activation_function\n",
        "\n",
        "  def init_W(self, X, y):\n",
        "    np.random.seed(self.random_state)\n",
        "    no_labels = np.unique(y).size\n",
        "    if no_labels <= 2:\n",
        "      self.W = np.random.randn(X.shape[1]+1, 1)\n",
        "    else:\n",
        "      self.W = np.random.randn(X.shape[1]+1, no_labels)\n",
        "  \n",
        "  def init_function(self, y):\n",
        "    if self.function == None: \n",
        "      if np.unique(y).size == 2:\n",
        "        self.function = \"sigmoid\"\n",
        "      else:\n",
        "        self.function = \"softmax\"\n",
        "\n",
        "  def gradient(self, X, y, prediction):\n",
        "    if self.function == \"sigmoid\":\n",
        "      return np.mean((prediction - y)*X, axis=0).reshape(-1,1)\n",
        "    else:\n",
        "      a = prediction - y\n",
        "      gradient = np.zeros([X.shape[1],y.shape[1]])\n",
        "      for i in range(y.shape[1]):\n",
        "        gradient[:,i:i+1] = np.mean(a[:,i:i+1]*X, axis=0).reshape(-1,1)\n",
        "      return gradient\n",
        "\n",
        "  def updateWeights(self, theta_old, gradient, learning_rate):\n",
        "    return theta_old - learning_rate*gradient\n",
        "\n",
        "  def computeLoss(self, X, y, prediction):\n",
        "    if self.function == \"sigmoid\":\n",
        "      return -np.mean((y*np.log(prediction) + (1-y)*np.log(1-prediction)), axis=0)\n",
        "    else:\n",
        "      for i in range(y.shape[1]):\n",
        "        error = (-1*y) * np.log(prediction)\n",
        "        error = np.sum(np.mean(error, axis=0))\n",
        "      return error\n",
        "\n",
        "  def prediction(self, X):\n",
        "    Z = X.dot(self.W)\n",
        "    if self.function == \"sigmoid\":\n",
        "      return sigmoid(Z)\n",
        "    return softmax(Z)\n",
        "\n",
        "  def BGD(self, X, y):\n",
        "    for epoch in range(self.max_iters):\n",
        "      pred = self.prediction(X)\n",
        "      grad = self.gradient(X, y, pred)\n",
        "      self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, pred))\n",
        "\n",
        "  def SGD(self, X, y):\n",
        "    for epoch in range(self.max_iters//X.shape[0]):\n",
        "      rd_id = np.random.permutation(X.shape[0])\n",
        "      for i in rd_id:\n",
        "        X_batch = X[j,:].reshape(1,-1)\n",
        "        y_batch = y[j,:].reshape(1,-1)\n",
        "        pred = self.prediction(X_batch)\n",
        "        grad = self.gradient(X_batch, y_batch, pred)\n",
        "        self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, self.prediction(X)))        \n",
        "  \n",
        "  def Mini_BGD(self, X, y, batch_size):\n",
        "    first_X = X.copy()\n",
        "    first_y = y.copy()\n",
        "    for epoch in range(self.max_iters//(X.shape[0]//self.batch_size + 1)):\n",
        "      indices = np.random.permutation(X.shape[0])\n",
        "      first_X = first_X[indices]\n",
        "      first_y = first_y[indices]\n",
        "\n",
        "      temp = batch_size\n",
        "      for j in range(0, X.shape[0], batch_size):    \n",
        "        X_batch = first_X[j:temp, :]\n",
        "        y_batch = first_y[j:temp,:]\n",
        "        pred = self.prediction(X_batch)\n",
        "        grad = self.gradient(X_batch, y_batch, pred)\n",
        "        self.W = self.updateWeights(self.W, grad, self.lr)\n",
        "\n",
        "        temp = temp + batch_size if X.shape[0]-temp >= batch_size else X.shape[0]\n",
        "      self.weights.append(self.W)\n",
        "      self.loss_values.append(self.computeLoss(X, y, self.prediction(X)))\n",
        "    \n",
        "  def fit(self, X, y):\n",
        "    self.loss_values.clear()\n",
        "    self.weights.clear()\n",
        "    self.init_W(X, y)\n",
        "    self.init_function(y)\n",
        "\n",
        "    X1 = np.concatenate([np.ones([X.shape[0],1]), X], axis=1)\n",
        "    if self.function == \"softmax\":\n",
        "      y = onehot_encoder(y)\n",
        "    else:\n",
        "      y = y.reshape(-1, 1)\n",
        "    \n",
        "\n",
        "    if (self.batch_size == None) or (self.batch_size == X.shape[0]):\n",
        "      self.BGD(X1, y)\n",
        "    elif (self.batch_size > 1) and (self.batch_size < X.shape[0]):\n",
        "      self.Mini_BGD(X1, y, self.batch_size)\n",
        "    elif self.batch_size == 1:\n",
        "      self.SGD(X1, y)\n",
        "    else:\n",
        "      raise Exception(\"Batch size is impossible.\")\n",
        "\n",
        "  def predict(self, X):\n",
        "    X1 = np.concatenate([np.ones([X.shape[0],1]), X], axis=1)\n",
        "    Z = X1.dot(self.W)\n",
        "    if self.function == \"sigmoid\":\n",
        "      prediction = sigmoid(Z)\n",
        "      return np.where(prediction>0.5,1,0).reshape(1,-1)\n",
        "    else:\n",
        "      prediction = softmax(Z)\n",
        "      return np.argmax(prediction,axis=1)\n",
        "      \n",
        "\n",
        "  def coef_(self):\n",
        "    return self.W[1:,:]\n",
        "\n",
        "  def intercept_(self):\n",
        "    return self.W[1,:]\n",
        "\n",
        "  def score(self, X, y):\n",
        "    if self.function == \"sigmoid\":\n",
        "      y = y.reshape(1,-1).T\n",
        "    return np.mean(y==self.predict(X).T)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwIGBOPLq_Pi"
      },
      "source": [
        "def scaler(X_train, X_test):\n",
        "  max = np.max(X_train,axis=0)\n",
        "  min = np.min(X_train,axis=0)\n",
        "\n",
        "  X_train = (X_train-min)/(max-min)\n",
        "  X_test = (X_test-min)/(max-min)\n",
        "  return X_train, X_test\n",
        "\n",
        "def onehot_encoder(y):\n",
        "  y_onehot = np.zeros( (y.size, y.max() + 1), dtype=int)\n",
        "  y_onehot[np.arange(y.size), y.reshape(-1)] = 1\n",
        "  return y_onehot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfqCfZxBlH8X"
      },
      "source": [
        "### Bài tập 1. Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "-bwfGfQIk7UY",
        "outputId": "e45e4986-ebfc-4320-8236-40de45585c6e"
      },
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-03/heart.csv\")\n",
        "data.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>148</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "      <td>294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>263</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>172</td>\n",
              "      <td>199</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>162</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
              "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
              "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
              "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
              "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
              "5   57    1   0       140   192    0  ...      0      0.4      1   0     1       1\n",
              "6   56    0   1       140   294    0  ...      0      1.3      1   0     2       1\n",
              "7   44    1   1       120   263    0  ...      0      0.0      2   0     3       1\n",
              "8   52    1   2       172   199    1  ...      0      0.5      2   0     3       1\n",
              "9   57    1   2       150   168    0  ...      0      1.6      2   0     2       1\n",
              "\n",
              "[10 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTagCWN-m_qj"
      },
      "source": [
        "# Split data, 33% for test and 67% for train\n",
        "X = data.drop(['target'], axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.33, random_state=42)\n",
        "\n",
        "# Scale data using min-max method\n",
        "X_train, X_test = scaler(X_train, X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bZs66Y1nNzY",
        "outputId": "4be6d943-e11b-4629-882b-61b2b47bbf6f"
      },
      "source": [
        "lr_bgd = Logistic_Regression(learning_rate=0.1, max_iters=30000, random_state=10)\n",
        "lr_bgd.fit(X_train, y_train)\n",
        "for i in range(0, len(lr_bgd.loss_values), 1000):\n",
        "  print(\"Loss at iter {}: {}\".format(i, lr_bgd.loss_values[i]))\n",
        "print(\"\\n\", \"-\"*40, \"\\n\")\n",
        "print(\">> Final loss: \", lr_bgd.loss_values[-1])\n",
        "print(\">> Final W: \\n\", lr_bgd.W)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at iter 0: [0.99859025]\n",
            "Loss at iter 1000: [0.34206429]\n",
            "Loss at iter 2000: [0.32313667]\n",
            "Loss at iter 3000: [0.31577836]\n",
            "Loss at iter 4000: [0.31207608]\n",
            "Loss at iter 5000: [0.31000355]\n",
            "Loss at iter 6000: [0.30877428]\n",
            "Loss at iter 7000: [0.30801748]\n",
            "Loss at iter 8000: [0.30753881]\n",
            "Loss at iter 9000: [0.30722959]\n",
            "Loss at iter 10000: [0.3070263]\n",
            "Loss at iter 11000: [0.30689058]\n",
            "Loss at iter 12000: [0.30679871]\n",
            "Loss at iter 13000: [0.30673572]\n",
            "Loss at iter 14000: [0.306692]\n",
            "Loss at iter 15000: [0.30666127]\n",
            "Loss at iter 16000: [0.30663943]\n",
            "Loss at iter 17000: [0.30662371]\n",
            "Loss at iter 18000: [0.30661226]\n",
            "Loss at iter 19000: [0.30660382]\n",
            "Loss at iter 20000: [0.30659753]\n",
            "Loss at iter 21000: [0.30659278]\n",
            "Loss at iter 22000: [0.30658917]\n",
            "Loss at iter 23000: [0.30658638]\n",
            "Loss at iter 24000: [0.30658421]\n",
            "Loss at iter 25000: [0.30658251]\n",
            "Loss at iter 26000: [0.30658116]\n",
            "Loss at iter 27000: [0.30658008]\n",
            "Loss at iter 28000: [0.30657922]\n",
            "Loss at iter 29000: [0.30657852]\n",
            "\n",
            " ---------------------------------------- \n",
            "\n",
            ">> Final loss:  [0.30657795]\n",
            ">> Final W: \n",
            " [[ 2.19167282]\n",
            " [ 1.00994856]\n",
            " [-1.45316221]\n",
            " [ 3.00421922]\n",
            " [-0.96587053]\n",
            " [-0.82193036]\n",
            " [ 0.32264186]\n",
            " [ 1.31644471]\n",
            " [ 1.92340993]\n",
            " [-1.16824963]\n",
            " [-2.52225591]\n",
            " [ 2.12561531]\n",
            " [-5.00528297]\n",
            " [-4.28452415]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "byQlAlH0oyYD",
        "outputId": "4c329f63-558f-422a-9dc4-b24221459054"
      },
      "source": [
        "# Plot loss versus interations\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(lr_bgd.loss_values)\n",
        "ax.set_title('Loss versus iterations')\n",
        "ax.set(xlabel='iterations', ylabel='loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xdZX3v8c93LpmBZCAkGRCSQLhEFFtuRrTVKvUKlJIWb6G1aqty2hrrrZdQLUVOW63W6zHVUutLQSQiSk96miNo8VblkuEqgRMYIpcMlwwhCQkhl5n5nT/Ws4e1d9YMk2TW7BnW9/167desvdaz1/qtWcn+7vU8e9ZSRGBmZtXV0uwCzMysuRwEZmYV5yAwM6s4B4GZWcU5CMzMKs5BYGZWcQ4CswkmaY2k05u4/SMlbZPU2qwabHJxENi4kXS/pNc2u47JLiJeFBE/ApB0kaRvlLm9xuMSEQ9GxIyIGCxzuzZ1OAjMEkltza5hb03Fmm3ycRBY6SR1SPqcpIfT43OSOtKyOZL+j6TNkp6Q9FNJLWnZX0nqk7RV0lpJrylY90slPZrv5pD0u5LuSNMtkpZJuk/SRklXSpqVli2QFJLeJelB4DpJnZK+kdpulrRa0mGpfd0n6/yn+dFeV1Dz/ZJeK+kM4K+Bt6aumtvT8oMl/ZukR9L+/11t/yS9U9LPJH1W0kbgIknHSroubftxSZdLmpnaXwYcCfxH2sZf5va7LbU5QtLK9PvvlfSehn28UtKl6TiskbQot/xZj5FNfg4CmwgfAV4GnAycBJwGfDQt+zCwHugGDiN7YwxJxwNLgZdERBfwBuD+xhVHxI3AU8Crc7N/D/hmmn4f8DvAq4AjgE3A8obVvAp4YdrGO4CDgfnAbOCPgafHsI97/bqI+B7wD8C3UlfNSWnR14AB4DjgFOD1wLtzL30psI7s9/X3gICPp/17YarhorSNPwAeBH47beOTBaWsIDsGRwBvAv5BUv73eU5qMxNYCXwRYKzHyCY/B4FNhN8HLo6IDRHRD3wM+IO0bDdwOHBUROyOiJ9GdgGsQaADOEFSe0TcHxH3jbD+K4DzACR1AWeleZC9IX8kItZHxE6yN8g3NXSpXBQRT0XE06me2cBxETEYETdHxJNj2Md9fV2ddBZxFvCBVNMG4LPAklyzhyPif0XEQEQ8HRG9EfH9iNiZfr+fIQu3sWxvPvBy4K8iYkdE3AZ8BXh7rtl/R8SqNKZwGVmYw94dI5vEHAQ2EY4AHsg9fyDNA/gU0AtcK2mdpGUAEdELfIDsjXuDpBWSjqDYN4FzU3fTucAtEVHb3lHA1am7ZjNwN9kbWL7b5qHc9GXANcCK1I31SUntY9jHfX1do6OAduCRXM3/Ahw6Qr1IOiz9fvokPQl8A5gzxu0dATwREVtz8x4A5uaeP5qb3g50Smrby2Nkk5iDwCbCw2RvcDVHpnlExNaI+HBEHEPWBfGhWj9zRHwzIl6RXhvAPxatPCLuInvzOpP6biHI3jTPjIiZuUdnRPTlV5Fb1+6I+FhEnAD8OnA2z3w6fgo4MPe6543xdaNpvPzvQ8BOYE6u3oMi4kWjvOYf0rxfjYiDgLeRdReN1D7vYWBWOpOqORLoG6F9ffFjPEY2uTkIbLy1p4HT2qONrJvmo5K6Jc0BLiT71IqksyUdJ0nAFrJP60OSjpf06vQpfwdZf/vQKNv9JvB+4JXAt3Pzvwz8vaSj0va6JS0eaSWSflPSr6bB2SfJunxq270NWCKpPQ2YvmmMrxvNY8ACpQHyiHgEuBb4tKSDlA12HytptK6eLmAbsEXSXOAvCrZxTNELI+Ih4OfAx9PxOhF4F+n4jGYfjpFNUg4CG2+ryN4Qao+LgL8DeoA7gF8At6R5AAuBH5C9kV0P/HNE/JCs7/kTwONkXROHAheMst0ryPrFr4uIx3PzP082wHmtpK3ADWSDrSN5HnAV2Zv53cCPybp9AP4GOJZswPlj1J95jPa60dRCa6OkW9L024FpwF1pW1eRjaOM5GPAqWRB+p/AdxuWf5wsiDdL+vOC158HLCA7O7ga+NuI+MEYat/bY2STlHxjGjOzavMZgZlZxTkIzMwqzkFgZlZxDgIzs4qbchesmjNnTixYsKDZZZiZTSk333zz4xHRXbRsygXBggUL6OnpaXYZZmZTiqQHRlrmriEzs4pzEJiZVZyDwMys4hwEZmYV5yAwM6u40oJA0lclbZB05wjLJekL6dZ4d0g6taxazMxsZGWeEXwNOGOU5WeSXXlyIXA+8KUSazEzsxGUFgQR8RPgiVGaLAYujcwNwExJo11qd7+svv8JPnPtWnYN+HLpZmZ5zRwjmEv9LffWU397vGGSzpfUI6mnv79/nzZ2ywOb+MJ1vQwMOQjMzPKmxGBxRFwSEYsiYlF3d+FfSO/FusapKDOz54hmBkEfMD/3fB5jvE/qvpCevY2ZWRU1MwhWAm9P3x56GbAl3a/VzMwmUGkXnZN0BXA6MEfSeuBvgXaAiPgy2b1tzwJ6ge3AH5ZVS557hszM6pUWBBFx3rMsD+C9ZW2/kXDfkJlZkSkxWDyewqPFZmZ1KhMEHiw2MytWmSAwM7NilQsCdwyZmdWrXBCYmVm9ygWBx4rNzOpVJgjk0WIzs0KVCQIzMytWvSBw15CZWZ3KBIE7hszMilUmCGrCpwRmZnUqEwQeKzYzK1aZIKjx10fNzOpVJgh8QmBmVqwyQWBmZsUqFwTuGTIzq1eZIPBfFpuZFatMENT4xjRmZvUqEwQ+ITAzK1ZqEEg6Q9JaSb2SlhUsP0rSf0m6Q9KPJM0rsx4zM9tTaUEgqRVYDpwJnACcJ+mEhmb/BFwaEScCFwMfL6ueGncMmZnVK/OM4DSgNyLWRcQuYAWwuKHNCcB1afqHBcvHjXuGzMyKlRkEc4GHcs/Xp3l5twPnpunfBbokzW5ckaTzJfVI6unv79+vojxWbGZWr9mDxX8OvErSrcCrgD5gsLFRRFwSEYsiYlF3d/e+bcmjxWZmhdpKXHcfMD/3fF6aNywiHiadEUiaAbwxIjaXWJOZmTUo84xgNbBQ0tGSpgFLgJX5BpLmSKrVcAHw1RLrAXwZajOzRqUFQUQMAEuBa4C7gSsjYo2kiyWdk5qdDqyVdA9wGPD3ZdXjjiEzs2Jldg0REauAVQ3zLsxNXwVcVWYNexY1oVszM5v0mj1YPGE8VmxmVqwyQVDjEwIzs3qVCQJ5lMDMrFBlgsDMzIpVLgj8l8VmZvUqEwQeLDYzK1aZIKjxH5SZmdWrTBD4hMDMrFhlgsDMzIpVLgg8WGxmVq8yQeDBYjOzYpUJghqfEJiZ1atMEPgvi83MilUmCMzMrFjlgiA8WmxmVqc6QeCeITOzQtUJgsQnBGZm9SoTBD4hMDMrVpkgMDOzYpUJAvkvyszMCpUaBJLOkLRWUq+kZQXLj5T0Q0m3SrpD0lll1mNmZnsqLQgktQLLgTOBE4DzJJ3Q0OyjwJURcQqwBPjnsuqp8WCxmVm9Ms8ITgN6I2JdROwCVgCLG9oEcFCaPhh4uKxi3DFkZlaszCCYCzyUe74+zcu7CHibpPXAKuB9RSuSdL6kHkk9/f39+1WUb0xjZlav2YPF5wFfi4h5wFnAZZL2qCkiLomIRRGxqLu7e5825LFiM7NiZQZBHzA/93xempf3LuBKgIi4HugE5pRYk5mZNSgzCFYDCyUdLWka2WDwyoY2DwKvAZD0QrIg2L++n2fhwWIzs3qlBUFEDABLgWuAu8m+HbRG0sWSzknNPgy8R9LtwBXAO6Okq8K5a8jMrFhbmSuPiFVkg8D5eRfmpu8CXl5mDXvUNJEbMzObApo9WDxhfGMaM7NilQkCMzMrVrkg8I1pzMzqVSYIPFhsZlasMkFQ4/MBM7N6lQsCMzOrV7kg8BCBmVm9ygSBb0xjZlasMkFgZmbFKhgE7hsyM8urTBC4Y8jMrFhlgqDGg8VmZvUqEwQeKzYzK1aZIDAzs2KVCwL3DJmZ1atMEPgy1GZmxSoTBDUeLDYzq1eZIPBgsZlZscoEgZmZFSs1CCSdIWmtpF5JywqWf1bSbelxj6TNZdYDEB4uNjOrU9rN6yW1AsuB1wHrgdWSVqYb1gMQER/MtX8fcEpp9ZS1YjOzKa7MM4LTgN6IWBcRu4AVwOJR2p8HXFFiPYAHi83MGpUZBHOBh3LP16d5e5B0FHA0cF1ZxXiw2Mys2GQZLF4CXBURg0ULJZ0vqUdST39//35tyGcEZmb1ygyCPmB+7vm8NK/IEkbpFoqISyJiUUQs6u7u3sdyfEpgZlakzCBYDSyUdLSkaWRv9isbG0l6AXAIcH2JtZiZ2QhKC4KIGACWAtcAdwNXRsQaSRdLOifXdAmwImJiOm389VEzs3qlfX0UICJWAasa5l3Y8PyiMmuo8WCxmVmxyTJYPGE8WGxmVq8yQeATAjOzYmMKAknvl3SQMv8m6RZJry+7ODMzK99Yzwj+KCKeBF5P9g2fPwA+UVpVZmY2YcYaBLWelbOAyyJiDVOst0UeLTYzKzTWILhZ0rVkQXCNpC5gqLyyyuPBYjOzemP9+ui7gJOBdRGxXdIs4A/LK2v8+XzAzKzYWM8Ifg1YGxGbJb0N+CiwpbyyzMxsoow1CL4EbJd0EvBh4D7g0tKqKpH/stjMrN5Yg2AgXQJiMfDFiFgOdJVX1vjzWLGZWbGxjhFslXQB2ddGf0NSC9BeXlnl8WCxmVm9sZ4RvBXYSfb3BI+SXVL6U6VVVQKfEZiZFRtTEKQ3/8uBgyWdDeyIiCk6RmBmZnljvcTEW4CbgDcDbwFulPSmMgsbb/IXSM3MCo11jOAjwEsiYgOApG7gB8BVZRVmZmYTY6xjBC21EEg27sVrJ5UJuv+NmdmUMdYzgu9JuoZn7iv8VhpuODPpuWfIzKzQmIIgIv5C0huBl6dZl0TE1eWVVR6fD5iZ1RvzrSoj4jvAd0qspVQ+ITAzKzZqEEjaSvGHaAEREQeVUpWZmU2YUQd8I6IrIg4qeHSNJQQknSFpraReSctGaPMWSXdJWiPpm/u6I2PlsWIzs3pj7hraW5JageXA64D1wGpJKyPirlybhcAFwMsjYpOkQ0usp6xVm5lNaWV+BfQ0oDci1kXELmAF2UXr8t4DLI+ITQANX1EtiU8JzMzyygyCucBDuefr07y85wPPl/QzSTdIOqNoRZLOl9Qjqae/v3+fivH5gJlZsWb/UVgbsBA4HTgP+FdJMxsbRcQlEbEoIhZ1d3dPcIlmZs9tZQZBHzA/93xempe3HlgZEbsj4pfAPWTBUBoPFpuZ1SszCFYDCyUdLWkasARY2dDm38nOBpA0h6yraF0ZxXis2MysWGlBEBEDwFLgGuBu4MqIWCPpYknnpGbXABsl3QX8EPiLiNhYVk3goWIzs0alfX0UICJW0XBNooi4MDcdwIfSo1StLdkpweCQo8DMLK/Zg8UTpq0l21UHgZlZvcoEQe2MYMBBYGZWp3JBMDg01ORKzMwml8oEQdtwEDS5EDOzSaYyQeAzAjOzYpUJgjaPEZiZFapMELT466NmZoUqEwRtDgIzs0KVCQJ/fdTMrFjlgsBnBGZm9SoXBD4jMDOrV5kgqF1iYshBYGZWpzJB4DMCM7NilQmC9tYsCHb7T4vNzOpUJgg62loB2LF7sMmVmJlNLpUJgtYW0d4qdg74jMDMLK8yQQDQ2dbqMwIzswaVCoKO9lZ27PYZgZlZXqWCoLO9hZ0+IzAzq1OxIGhlx4CDwMwsr9QgkHSGpLWSeiUtK1j+Tkn9km5Lj3eXWU9HW4u7hszMGrSVtWJJrcBy4HXAemC1pJURcVdD029FxNKy6sjrbPdgsZlZozLPCE4DeiNiXUTsAlYAi0vc3rPqbG/x10fNzBqUGQRzgYdyz9eneY3eKOkOSVdJml+0IknnS+qR1NPf37/PBfnro2Zme2r2YPF/AAsi4kTg+8DXixpFxCURsSgiFnV3d+/zxtw1ZGa2pzKDoA/If8Kfl+YNi4iNEbEzPf0K8OIS66Gj3YPFZmaNygyC1cBCSUdLmgYsAVbmG0g6PPf0HODuEuvxGYGZWYHSvjUUEQOSlgLXAK3AVyNijaSLgZ6IWAn8maRzgAHgCeCdZdUD0NXRxradA2VuwsxsyiktCAAiYhWwqmHehbnpC4ALyqwhb3pHGzsHhtg9OER7a7OHR8zMJodKvRvO6Mhy7ymfFZiZDatWEHRmQbB1h4PAzKymUkHQlc4IPE5gZvaMSgXBdHcNmZntoVJBMNw15CAwMxtWqSAY7hryGIGZ2bBKBUHtjMBdQ2Zmz6hUENTGCPytITOzZ1QqCGZMa6O1RWx5enezSzEzmzQqFQQtLeKQA9vZ+NSuZpdiZjZpVCoIAGZNn8YTT+189oZmZhVR0SDwGYGZWU3lgmD29A53DZmZ5VQuCHxGYGZWr5JBsHn7bgYGfacyMzOoYBDM6eoAcPeQmVlSuSCYO7MTgL7NTze5EjOzyaGCQXAgAOs3OQjMzKCKQXDIAQD0OQjMzICSg0DSGZLWSuqVtGyUdm+UFJIWlVkPZLernHlgO+s3bS97U2ZmU0JpQSCpFVgOnAmcAJwn6YSCdl3A+4Eby6ql0bxDDnDXkJlZUuYZwWlAb0Ssi4hdwApgcUG7/wn8I7CjxFrqHDNnBr0btk3U5szMJrUyg2Au8FDu+fo0b5ikU4H5EfGfo61I0vmSeiT19Pf373dhxz+vi77NT7N1h69CambWtMFiSS3AZ4APP1vbiLgkIhZFxKLu7u793vbxh3UBcM9jW/d7XWZmU12ZQdAHzM89n5fm1XQBvwL8SNL9wMuAlRMxYHz887IguPsRB4GZWZlBsBpYKOloSdOAJcDK2sKI2BIRcyJiQUQsAG4AzomInhJrArLB4tnTp3Hrg5vL3pSZ2aRXWhBExACwFLgGuBu4MiLWSLpY0jllbXcsJPGSBbO48Zcbm1mGmdmk0FbmyiNiFbCqYd6FI7Q9vcxaGp129Cy+t+ZR+jY/zdyZB0zkps3MJpXK/WVxza8fNxuAH63d0ORKzMyaq7JBcPxhXSyYfSDfu/PRZpdiZtZUlQ0CSZz5q4fz8/s2snGb72FsZtVV2SAAOPeUuQwOBStWP/Tsjc3MnqMqHQQLD+vi14+dzTdueIDdvmOZmVVUpYMA4D2/cQyPbNnBipsebHYpZmZNUfkgOP34bl569Cw+94N72fK0rz1kZtVT+SCQxN+cfQKbn97NRSvXNLscM7MJV/kgAPiVuQfzvlcfx9W39nFljweOzaxaHATJ0t88jpcfN5u//u4v+Mk9+3+pazOzqcJBkLS1tvClt72Y4w6dwbsv7fEfmplZZTgIcg7qbOeK97yMFx1xEH9y+c185vv3MOCvlZrZc5yDoMEh06dx+btfyrmnzOML/3Uvb/6X6/nF+i3NLsvMrDQOggIHTmvj0285ic8vOZmHntjOOcv/mw996zbf0czMnpNKvQz1VLf45Ln85gsO5YvX9XLZ9Q/w3Vv7OP34bt784vm85oWH0tne2uwSzcz2myKi2TXslUWLFkVPT+k3MdvDpqd28fXr72fFTQ/x6JM76Ops49UvOJTTj+/mlQu7mT2jY8JrMjMbK0k3R0ThrYAdBHtpcCi4/r6NXH1rHz9au4GNT+1Cgucf2sUpR87k1CMP4cT5B3PMnBlMa3PPm5lNDg6CkgwNBXc+vIUfr+3n5gc3ceuDm4cvU9HaIhbMPpCFh3ax8LAZzD/kQOYecgBzZx7A4TM76Whzt5KZTZzRgsBjBPuhpUWcOG8mJ86bCUBEsO7xp7izbwv3PraNezds5Z7HtnLtXY8ylMtbCbpndHDYQZ3Mmj6N2TOmMWdGB7OnT2P2jA5mTW+nq7OdGR1tdHW20dXRzozONlpb1KQ9NbPnMgfBOJLEsd0zOLZ7Rt38XQNDPLplB+s3b6dv09P0bX6avk1P8/i2nWx8ahe9G7bx+Lad7BwY/W8WDpzWSldnGzM62uhsb02PFjrbsumO9pZsXls2vyP9bGttob1VtLW00NYi2lqVzWvJfra1ivaW9LPWLv1sbcn2q0WiRWQ/W56ZlqB1eLlQC/VtVd9WcpiZTTalBoGkM4DPA63AVyLiEw3L/xh4LzAIbAPOj4i7yqypGaa1tXDk7AM5cvaBI7aJCLbvGmTjtl08sX0XW3fsZtuOAbbuGGDrzoE0vZttO7PnO3YNsmNgkB27h9i8fTc7dmfTO9O8HbsHGRiafN1+daGAIOVCLR6G5w9P1+ZreDr/mlqwjNRWuRep7nXDcwtfT+7145VduT3Y93WMQy3jFcXjEerjUsu4HZ9xWEfJH3Te/5qF/PZJR4z7eksLAkmtwHLgdcB6YLWklQ1v9N+MiC+n9ucAnwHOKKumyUwS0zvamN7RNmpg7I2BwSF2DgwxMBjsHko/B4cYHAoGhobYPRh1ywYGh9g9lH4OBoNDwVBkjwiGn0eQ5sNgBBHB0FD2fLht7nW1ZcNt02trw1PB8ERtikivTbP3aJsf2oqI3Ov2bBN17XPrza1zxG2NU5aOx2rGYzxvvD4ajMfQ4mT5ncA4/V4m4HPXwQe0l7LeMs8ITgN6I2IdgKQVwGJgOAgi4slc++lMyK+yOrJuH39zycxGV2YQzAXy13ReD7y0sZGk9wIfAqYBry5akaTzgfMBjjzyyHEv1Mysypr+cTEilkfEscBfAR8doc0lEbEoIhZ1d3dPbIFmZs9xZQZBHzA/93xemjeSFcDvlFiPmZkVKDMIVgMLJR0taRqwBFiZbyBpYe7pbwH3lliPmZkVKG2MICIGJC0FriH7+uhXI2KNpIuBnohYCSyV9FpgN7AJeEdZ9ZiZWbFS/44gIlYBqxrmXZibfn+Z2zczs2fX9MFiMzNrLgeBmVnFTbmrj0rqBx7Yx5fPAR4fx3Kayfsy+TxX9gO8L5PV/uzLURFR+P37KRcE+0NSz0iXYZ1qvC+Tz3NlP8D7MlmVtS/uGjIzqzgHgZlZxVUtCC5pdgHjyPsy+TxX9gO8L5NVKftSqTECMzPbU9XOCMzMrIGDwMys4ioTBJLOkLRWUq+kZc2up4ik+yX9QtJtknrSvFmSvi/p3vTzkDRfkr6Q9ucOSafm1vOO1P5eSRNy/SZJX5W0QdKduXnjVrukF6ffTW96bWn3BBxhXy6S1JeOzW2SzsotuyDVtVbSG3LzC//NpQsx3pjmfytdlLGM/Zgv6YeS7pK0RtL70/wpd1xG2ZepeFw6Jd0k6fa0Lx8bbfuSOtLz3rR8wb7u44gi3T7wufwgu+jdfcAxZDfAuR04odl1FdR5PzCnYd4ngWVpehnwj2n6LOD/kt1q9WXAjWn+LGBd+nlImj5kAmp/JXAqcGcZtQM3pbZKrz1zgvflIuDPC9qekP49dQBHp39nraP9mwOuBJak6S8Df1LSfhwOnJqmu4B7Ur1T7riMsi9T8bgImJGm24Eb0++wcPvAnwJfTtNLgG/t6z6O9KjKGcHwbTMjYhfZvQ8WN7mmsVoMfD1Nf51n7tmwGLg0MjcAMyUdDrwB+H5EPBERm4DvMwH3gY6InwBPlFF7WnZQRNwQ2f+ASynx3hUj7MtIFgMrImJnRPwS6CX791b4by59Yn41cFV6ff73Mq4i4pGIuCVNbwXuJrtz4JQ7LqPsy0gm83GJiNiWnranR4yy/fzxugp4Tap3r/ZxtJqqEgRFt80c7R9RswRwraSbld2eE+CwiHgkTT8KHJamR9qnybSv41X73DTdOH+iLU1dJl+tdaew9/syG9gcEQMN80uVuhNOIfv0OaWPS8O+wBQ8LpJaJd0GbCAL1vtG2f5wzWn5llTvuL0HVCUIpopXRMSpwJnAeyW9Mr8wfeqakt/3ncq1J18CjgVOBh4BPt3ccsZO0gzgO8AHIuLJ/LKpdlwK9mVKHpeIGIyIk8nu3Hga8IJm1lOVINjb22Y2RUT0pZ8bgKvJ/oE8lk7BST83pOYj7dNk2tfxqr0vTTfOnzAR8Vj6zzsE/CvZsYG935eNZF0ubQ3zSyGpneyN8/KI+G6aPSWPS9G+TNXjUhMRm4EfAr82yvaHa07LD071jt97QBmDIZPtQXYDnnVkAyq1wZMXNbuuhhqnA1256Z+T9e1/ivqBvU+m6d+ifmDvpjR/FvBLskG9Q9L0rAnahwXUD7COW+3sOSh51gTvy+G56Q+S9c0CvIj6Abt1ZIN1I/6bA75N/aDgn5a0DyLrt/9cw/wpd1xG2ZepeFy6gZlp+gDgp8DZI20feC/1g8VX7us+jlhTmf+ZJtOD7BsR95D1xX2k2fUU1HdMOmC3A2tqNZL1Bf4X2f2cf5D7DyhgedqfXwCLcuv6I7KBo17gDyeo/ivITs13k/VJvms8awcWAXem13yR9FfxE7gvl6Va7yC793b+Degjqa615L41M9K/uXSsb0r7+G2go6T9eAVZt88dwG3pcdZUPC6j7MtUPC4nAremmu8ELhxt+0Bnet6blh+zr/s40sOXmDAzq7iqjBGYmdkIHARmZhXnIDAzqzgHgZlZxTkIzMwqzkFglSPp5+nnAkm/N87r/uuibZlNZv76qFWWpNPJrlx59l68pi2euR5M0fJtETFjPOozmyg+I7DKkVS78uMngN9I17H/YLoQ2KckrU4XMfsfqf3pkn4qaSVwV5r37+nigGtqFwiU9AnggLS+y/PbUuZTku5M1+9/a27dP5J0laT/J+ny2jX9JX0iXX//Dkn/NJG/I6uWtmdvYvactYzcGUF6Q98SES+R1AH8TNK1qe2pwK9EdrlfgD+KiCckHQCslvSdiFgmaWlkFxNrdC7ZhdFOAuak1/wkLTuF7HIBDwM/A14u6W7gd4EXRERImjnue2+W+IzA7BmvB96eLg98I9mlGBamZTflQgDgzyTdDtxAdoGvhYzuFcAVkV0g7THgx8BLcuteH9mF024ju87RFmAH8G+SzgW27/femY3AQWD2DAHvi4iT0+PoiKidETw13CgbW3gt8GsRcRLZdWM692O7O3PTg0BtHOI0shuRnA18bz/WbzYqB4FV2Vay2x7WXAP8SbrcMZKeL2l6wesOBjZFxHZJLyC7+tNFWy4AAACnSURBVGbN7trrG/wUeGsah+gmux3mTSMVlq67f3BErCK7quZJe7NjZnvDYwRWZXcAg6mL52vA58m6ZW5JA7b9FN+u8HvAH6d+/LVk3UM1lwB3SLolIn4/N/9qsmvO3052Fc2/jIhHU5AU6QL+t6ROsjOVD+3bLpo9O3991Mys4tw1ZGZWcQ4CM7OKcxCYmVWcg8DMrOIcBGZmFecgMDOrOAeBmVnF/X/CbYw8t89KTAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80JktnBPYYwF",
        "outputId": "8fbfe10d-d055-49b7-d07b-a3a51614385e"
      },
      "source": [
        "print(\">> Original label: \\n\", y_test)\n",
        "y_pred = lr_bgd.predict(X_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred)\n",
        "print(\"\\n>> Score: \", lr_bgd.score(X_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "\n",
            ">> Predicted label: \n",
            " [[0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0\n",
            "  1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            "  1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0]]\n",
            "\n",
            ">> Score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35R_63EapJDF"
      },
      "source": [
        "**Kết quả bài toán Logistic Regression với bộ dữ liệu *Heart* chạy bằng thư viện scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRjSP4T40ZFn",
        "outputId": "07ccb78c-421b-47f4-cbdf-1dd91382be7b"
      },
      "source": [
        "lr = LogisticRegression(random_state=10)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "score = lr.score(X_test, y_test)\n",
        "\n",
        "print(\">> Original label: \\n\", y_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred)\n",
        "print(\"\\n>> Score: \", score)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "\n",
            ">> Predicted label: \n",
            " [0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0]\n",
            "\n",
            ">> Score:  0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2miKilgK5-Kg"
      },
      "source": [
        "### Bài tập 2. Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFNlJb7D6Sm8"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "y = iris_data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "X_train, X_test = scaler(X_train, X_test)\n",
        "\n",
        "y_test_enc = onehot_encoder(y_test)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apURiLHMpqQV",
        "outputId": "5fe0747a-370e-4766-9d59-2be9d68cde6b"
      },
      "source": [
        "lr_bgd = Logistic_Regression(learning_rate=0.01, max_iters=200000, random_state=10)\n",
        "lr_bgd.fit(X_train, y_train)\n",
        "for i in range(0, len(lr_bgd.loss_values), 2000):\n",
        "  print(\"Loss at iter {}: {}\".format(i, lr_bgd.loss_values[i]))\n",
        "print(\"\\n\", \"-\"*40, \"\\n\")\n",
        "print(\">> Final loss: \", lr_bgd.loss_values[-1])\n",
        "print(\">> Final W: \\n\", lr_bgd.W)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at iter 0: 1.4556966295110803\n",
            "Loss at iter 2000: 0.5347940280234592\n",
            "Loss at iter 4000: 0.45120846702235773\n",
            "Loss at iter 6000: 0.4023456612588975\n",
            "Loss at iter 8000: 0.3687728826088267\n",
            "Loss at iter 10000: 0.34356418857827625\n",
            "Loss at iter 12000: 0.3235391508439497\n",
            "Loss at iter 14000: 0.3070039834842857\n",
            "Loss at iter 16000: 0.2929636069989429\n",
            "Loss at iter 18000: 0.28079025624021947\n",
            "Loss at iter 20000: 0.27006547004337517\n",
            "Loss at iter 22000: 0.26049764345426313\n",
            "Loss at iter 24000: 0.2518759565630183\n",
            "Loss at iter 26000: 0.2440431897708213\n",
            "Loss at iter 28000: 0.23687894857665817\n",
            "Loss at iter 30000: 0.23028891292098014\n",
            "Loss at iter 32000: 0.22419771914379316\n",
            "Loss at iter 34000: 0.21854411057224526\n",
            "Loss at iter 36000: 0.2132775489207243\n",
            "Loss at iter 38000: 0.20835579210003435\n",
            "Loss at iter 40000: 0.2037431269815665\n",
            "Loss at iter 42000: 0.1994090558074353\n",
            "Loss at iter 44000: 0.1953273030932805\n",
            "Loss at iter 46000: 0.19147505309159485\n",
            "Loss at iter 48000: 0.18783235590606315\n",
            "Loss at iter 50000: 0.18438165888545335\n",
            "Loss at iter 52000: 0.18110743241724175\n",
            "Loss at iter 54000: 0.17799586780238483\n",
            "Loss at iter 56000: 0.17503463085296644\n",
            "Loss at iter 58000: 0.17221265906494904\n",
            "Loss at iter 60000: 0.1695199932335535\n",
            "Loss at iter 62000: 0.16694763656587736\n",
            "Loss at iter 64000: 0.16448743595099913\n",
            "Loss at iter 66000: 0.16213198124002434\n",
            "Loss at iter 68000: 0.15987451928334584\n",
            "Loss at iter 70000: 0.15770888015089474\n",
            "Loss at iter 72000: 0.15562941348060888\n",
            "Loss at iter 74000: 0.1536309333017507\n",
            "Loss at iter 76000: 0.15170866999255211\n",
            "Loss at iter 78000: 0.14985822827757633\n",
            "Loss at iter 80000: 0.1480755503649669\n",
            "Loss at iter 82000: 0.1463568834792256\n",
            "Loss at iter 84000: 0.14469875117011546\n",
            "Loss at iter 86000: 0.14309792787941633\n",
            "Loss at iter 88000: 0.14155141632959056\n",
            "Loss at iter 90000: 0.14005642736590712\n",
            "Loss at iter 92000: 0.13861036193914675\n",
            "Loss at iter 94000: 0.1372107949620812\n",
            "Loss at iter 96000: 0.13585546081126246\n",
            "Loss at iter 98000: 0.1345422402777447\n",
            "Loss at iter 100000: 0.13326914879733825\n",
            "Loss at iter 102000: 0.13203432581375624\n",
            "Loss at iter 104000: 0.13083602514732168\n",
            "Loss at iter 106000: 0.12967260625831273\n",
            "Loss at iter 108000: 0.12854252630805887\n",
            "Loss at iter 110000: 0.1274443329329117\n",
            "Loss at iter 112000: 0.12637665765655184\n",
            "Loss at iter 114000: 0.12533820987500488\n",
            "Loss at iter 116000: 0.12432777135644808\n",
            "Loss at iter 118000: 0.12334419120458398\n",
            "Loss at iter 120000: 0.12238638124016746\n",
            "Loss at iter 122000: 0.12145331176035296\n",
            "Loss at iter 124000: 0.12054400763996569\n",
            "Loss at iter 126000: 0.11965754474268131\n",
            "Loss at iter 128000: 0.11879304661352094\n",
            "Loss at iter 130000: 0.11794968142706808\n",
            "Loss at iter 132000: 0.11712665916846657\n",
            "Loss at iter 134000: 0.1163232290266006\n",
            "Loss at iter 136000: 0.1155386769809336\n",
            "Loss at iter 138000: 0.11477232356532036\n",
            "Loss at iter 140000: 0.11402352179374126\n",
            "Loss at iter 142000: 0.11329165523436513\n",
            "Loss at iter 144000: 0.11257613621964535\n",
            "Loss at iter 146000: 0.11187640418130936\n",
            "Loss at iter 148000: 0.111191924100139\n",
            "Loss at iter 150000: 0.11052218506137262\n",
            "Loss at iter 152000: 0.10986669890738618\n",
            "Loss at iter 154000: 0.1092249989800605\n",
            "Loss at iter 156000: 0.10859663894592067\n",
            "Loss at iter 158000: 0.10798119169773057\n",
            "Loss at iter 160000: 0.10737824832677975\n",
            "Loss at iter 162000: 0.10678741716059054\n",
            "Loss at iter 164000: 0.10620832286122034\n",
            "Loss at iter 166000: 0.10564060557973824\n",
            "Loss at iter 168000: 0.10508392016282242\n",
            "Loss at iter 170000: 0.10453793540775383\n",
            "Loss at iter 172000: 0.1040023333623886\n",
            "Loss at iter 174000: 0.10347680866696196\n",
            "Loss at iter 176000: 0.1029610679348279\n",
            "Loss at iter 178000: 0.10245482916946827\n",
            "Loss at iter 180000: 0.10195782121530536\n",
            "Loss at iter 182000: 0.10146978324005464\n",
            "Loss at iter 184000: 0.10099046424651305\n",
            "Loss at iter 186000: 0.10051962261184857\n",
            "Loss at iter 188000: 0.10005702565259281\n",
            "Loss at iter 190000: 0.09960244921367709\n",
            "Loss at iter 192000: 0.09915567727997483\n",
            "Loss at iter 194000: 0.09871650160891773\n",
            "Loss at iter 196000: 0.09828472138286573\n",
            "Loss at iter 198000: 0.09786014287999838\n",
            "\n",
            " ---------------------------------------- \n",
            "\n",
            ">> Final loss:  0.09744278622193912\n",
            ">> Final W: \n",
            " [[  6.23630774   4.38762457 -10.12246713]\n",
            " [ -3.30976168   2.24377814   0.95885009]\n",
            " [  6.74014935  -0.88360805  -5.47818975]\n",
            " [ -8.42483765  -0.49326194  10.37956294]\n",
            " [ -8.46300758  -3.08026283  11.83510895]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zlkm9eF6GbEL",
        "outputId": "587edbc3-b836-46a9-83e2-a93ab83c6373"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(lr_bgd.loss_values)\n",
        "ax.set_title('Loss versus iterations')\n",
        "ax.set(xlabel='iterations', ylabel='loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxddZ3/8dcn92ZpkzRdku4taaGAVdYpCLLqKFP4MeDCaOsuKOpvcHR0VBz8ITobjj+dcWFEVIaHyK7i9DeDUh1AUAZoylJooVBqS/emW9omzXKTz++Pc+7tachdEnpyk5738/G4j3vu9yzfzz03ue+cJeeYuyMiIgJQUe4CRERk5FAoiIhIjkJBRERyFAoiIpKjUBARkRyFgoiI5CgURIaZma00s/PL2P9sM9tvZqly1SAjl0JBDhszW2dmby13HSOdu7/e3R8CMLPrzOyncfbX/3Nx91fcvc7de+PsV0YnhYJIyMzS5a5hsEZjzTKyKRQkdmZWbWb/amabw8e/mll1OK7RzP7TzPaY2S4ze8TMKsJxXzSzTWa2z8xWm9mfDrDsN5rZ1uiuEDN7h5mtCIcrzOxqM3vZzHaa2d1mNjEc12xmbmZXmNkrwANmVmNmPw2n3WNmy8xsSjj9IX9xR//KLzTfADWvM7O3mtlC4G+B94S7c54JxzeY2Y/NbEv4/v8++/7M7MNm9gcz+xcz2wlcZ2ZHm9kDYd87zOw2MxsfTn8rMBv4f2EfX4i873Q4zXQzWxKu/zVm9rF+7/FuM/tJ+DmsNLMFkfFFPyMZXRQKMhyuAc4ATgZOAk4HvhyO+xywEWgCphB8SbqZHQdcBZzm7vXAnwHr+i/Y3R8H2oG3RJrfC9weDn8KeDtwHjAd2A3c0G8x5wGvC/v4ENAAzAImAZ8ADpTwHgc9n7v/GvhH4K5wd85J4ahbgAxwDHAKcAHw0cisbwTWEqyvfwAM+Kfw/b0urOG6sI8PAK8Afx728c8DlHInwWcwHbgM+Eczi67PS8JpxgNLgO8BlPoZyeiiUJDh8D7ga+6+3d1bga8CHwjH9QDTgKPcvcfdH/Hggly9QDUw38wq3X2du7+cZ/l3AIsBzKweuChsg+DL+Rp33+juXQRflpf12+1ynbu3u/uBsJ5JwDHu3uvuy919bwnvcajzHSLcurgI+ExY03bgX4BFkck2u/t33T3j7gfcfY27/8bdu8L1+y2CoCulv1nAWcAX3b3T3Z8GfgR8MDLZ7939vvAYxK0EwQ6D+4xklFAoyHCYDqyPvF4ftgF8A1gDLDWztWZ2NYC7rwE+Q/Alvt3M7jSz6QzsduCd4S6pdwJPunu2v6OAe8NdOnuA5wm+zKK7djZEhm8F7gfuDHd1/bOZVZbwHoc6X39HAZXAlkjNPwAm56kXM5sSrp9NZrYX+CnQWGJ/04Fd7r4v0rYemBF5vTUy3AHUmFl6kJ+RjBIKBRkOmwm+7LJmh224+z53/5y7zyXYTfHZ7H5pd7/d3c8O53Xg6wMt3N1XEXyRXcihu44g+AK90N3HRx417r4puojIsnrc/avuPh94E3AxB/9qbgfGRuabWuJ8hfS/TPEGoAtojNQ7zt1fX2CefwzbTnD3ccD7CXYp5Zs+ajMwMdzCypoNbMoz/aHFl/gZyeihUJDDrTI86Jp9pAl25XzZzJrMrBG4luCvWczsYjM7xswMaCP4K77PzI4zs7eEf/13Euyf7yvQ7+3Ap4FzgXsi7TcC/2BmR4X9NZnZpfkWYmZvNrMTwgO7ewl2C2X7fRpYZGaV4cHWy0qcr5BtQLOFB9fdfQuwFPimmY2z4ED50WZWaHdQPbAfaDOzGcDnB+hj7kAzuvsG4FHgn8LP60TgCsLPp5AhfEYyCigU5HC7j+DLIfu4Dvh7oAVYATwLPBm2AcwDfkvwpfY/wL+5+4ME+6qvB3YQ7L6YDHypQL93EOxHf8Ddd0Tav01wcHSpme0DHiM4UJvPVOBnBF/szwO/I9g1BPB/gKMJDlZ/lUO3SArNV0g2wHaa2ZPh8AeBKmBV2NfPCI675PNV4FSCUP0v4Bf9xv8TQSjvMbO/GWD+xUAzwVbDvcBX3P23JdQ+2M9IRgHTTXZERCRLWwoiIpKjUBARkRyFgoiI5CgUREQkZ9RdTKuxsdGbm5vLXYaIyKiyfPnyHe7eVGy6URcKzc3NtLS0lLsMEZFRxczWF59Ku49ERCRCoSAiIjkKBRERyVEoiIhIjkJBRERyFAoiIpKjUBARkZzEhMLqrfv41tLV7NjfVe5SRERGrMSEwkvb9/GdB9awq7273KWIiIxYiQkFC+9OqNtHiIjkl5xQsOLTiIgkXWJCIcsL3sNcRCTZEhMK2Q0F7T4SEckvtlAws5vNbLuZPVdkutPMLGNml8VVS9BP8KxQEBHJL84thVuAhYUmMLMU8HVgaYx1ZHsDtPtIRKSQ2ELB3R8GdhWZ7FPAz4HtcdWRpS0FEZHiynZMwcxmAO8Avl/CtFeaWYuZtbS2tg6tvyHNJSKSLOU80PyvwBfdva/YhO5+k7svcPcFTU1F7yY3INM5qSIiRZXzdpwLgDvDL+tG4CIzy7j7L+PsVLuPRETyK1souPuc7LCZ3QL8Z5yBkDslVQeaRUTyii0UzOwO4Hyg0cw2Al8BKgHc/ca4+s1fT/CsLQURkfxiCwV3XzyIaT8cVx1ZuVCIuyMRkVEsQf/RnL0gnmJBRCSfxIQC2lIQESkqMaGgE1JFRIpLTChkae+RiEh+iQmFg/+8plQQEcknOaEQPmtLQUQkv+SEgg40i4gUlZxQ0D2aRUSKSk4o6PQjEZGiEhMKWfrnNRGR/BITCjr3SESkuMSEArognohIUYkJBdM9mkVEikpOKGj/kYhIUckJhfBZmSAikl9yQkHnpIqIFJWYUMjSgWYRkfwSEwoHL3OhVBARySc5oRA+a0tBRCS/5ISCLognIlJUbKFgZjeb2XYzey7P+PeZ2Qoze9bMHjWzk+KqJewR0GUuREQKiXNL4RZgYYHxfwTOc/cTgL8DboqxFm0piIiUIB3Xgt39YTNrLjD+0cjLx4CZcdUCukeziEgpRsoxhSuAX+UbaWZXmlmLmbW0tra+tp60qSAiklfZQ8HM3kwQCl/MN4273+TuC9x9QVNT01D7CZalVBARySu23UelMLMTgR8BF7r7zlj7Cp91nFlEJL+ybSmY2WzgF8AH3P3F+PsLnhUKIiL5xbalYGZ3AOcDjWa2EfgKUAng7jcC1wKTgH8Ld+1k3H1BbPXkLp0tIiL5xHn20eIi4z8KfDSu/vvT9fBERIor+4Hm4aZ/XhMRyS95oVDuAkRERrDEhIIONIuIFJecUNC910REikpOKGhLQUSkqOSFQnnLEBEZ0ZITCroknohIUYkJhSztPhIRyS8xoaB7NIuIFJecUAiftaUgIpJfckJBB5pFRIpKTCjoHs0iIsUlJhR0QTwRkeISEwoiIlJcYkJBB5pFRIpLTijoHs0iIkUlJxTCZ20piIjkl5xQ0AXxRESKSk4o6B7NIiJFJScUdEqqiEhRsYWCmd1sZtvN7Lk8483MvmNma8xshZmdGlctUfrnNRGR/OLcUrgFWFhg/IXAvPBxJfD9GGvJUSSIiOQXWyi4+8PArgKTXAr8xAOPAePNbFpc9ZjuxikiUlQ5jynMADZEXm8M22Kh/1MQESluVBxoNrMrzazFzFpaW1uHtozwWYcURETyK2cobAJmRV7PDNtexd1vcvcF7r6gqalpSJ3p0tkiIsWVMxSWAB8Mz0I6A2hz9y1xdaZ7NIuIFJeOa8FmdgdwPtBoZhuBrwCVAO5+I3AfcBGwBugAPhJXLVHafSQikl9soeDui4uMd+Av4+q/P92jWUSkuFFxoPlw0IFmEZHiEhMK6ECziEhRiQmF3IFmbSqIiOSVnFDQyUciIkUlJhSytJ0gIpJfYkJBB5pFRIpLTihkr32kVBARySs5oRA+KxJERPJLTijo5CMRkaKSEwq6R7OISFGJCQVdD09EpLjkhEJIB5pFRPJLTCjon9dERIpLTiiEz9pQEBHJLzmhoHs0i4gUlZxQCJ+1pSAikl9yQkGXzhYRKSo5oaBzUkVEikpMKGRp95GISH6JCQXdo1lEpLjEhEKWthRERPIrKRTM7NNmNs4CPzazJ83sghLmW2hmq81sjZldPcD42Wb2oJk9ZWYrzOyiobyJUqQrgk2Fvj6lgohIPqVuKVzu7nuBC4AJwAeA6wvNYGYp4AbgQmA+sNjM5veb7MvA3e5+CrAI+LdB1D4oqTAUMgoFEZG8Sg2F7Kk7FwG3uvtKil9i7nRgjbuvdfdu4E7g0n7TODAuHG4ANpdYz6CZGRUGfdp/JCKSV6mhsNzMlhKEwv1mVg/0FZlnBrAh8npj2BZ1HfB+M9sI3Ad8aqAFmdmVZtZiZi2tra0llvxqqQrTloKISAGlhsIVwNXAae7eAVQCHzkM/S8GbnH3mYRbIWb2qprc/SZ3X+DuC5qamobcWarCdExBRKSAUkPhTGC1u+8xs/cTHAtoKzLPJmBW5PXMsC3qCuBuAHf/H6AGaCyxpkFLmbYUREQKKTUUvg90mNlJwOeAl4GfFJlnGTDPzOaYWRXBgeQl/aZ5BfhTADN7HUEoDH3/UBEVFUavQkFEJK9SQyHjwd1pLgW+5+43APWFZnD3DHAVcD/wPMFZRivN7Gtmdkk42eeAj5nZM8AdwIc9xrvgpBUKIiIFpUucbp+ZfYngVNRzwv3+lcVmcvf7CA4gR9uujQyvAs4qvdzXJlVh9OrsIxGRvErdUngP0EXw/wpbCY4PfCO2qmKiA80iIoWVFAphENwGNJjZxUCnuxc7pjDi6ECziEhhpV7m4t3AE8BfAO8GHjezy+IsLA6plLYUREQKKfWYwjUE/6OwHcDMmoDfAj+Lq7A4aEtBRKSwUo8pVGQDIbRzEPOOGDrQLCJSWKlbCr82s/sJThuF4MDzfQWmH5FSFUZvr0JBRCSfkkLB3T9vZu/i4OmjN7n7vfGVFY9URYW2FERECih1SwF3/znw8xhriV2qAjK9xa7jJyKSXAVDwcz2wYD3rzTA3X3cAONGrDGVKboyCgURkXwKhoK7F7yUxWgztirNngM95S5DRGTEGnVnEL0WtdUp2rsy5S5DRGTESlQojK1K06FQEBHJK1GhUFuVor27t9xliIiMWMkKheo07V0ZYrw6t4jIqJaoUBg/tpJMn2trQUQkj0SFwqTaagB27OsqcyUiIiNTokKhsT4IhZ3tCgURkYEkKhQm1VYB0Lqvu8yViIiMTIkKhcY6bSmIiBSSqFCYGG4p7NCWgojIgGINBTNbaGarzWyNmV2dZ5p3m9kqM1tpZrfHWU9VuoKJtVVs3dsZZzciIqNWyVdJHSwzSwE3AG8DNgLLzGyJu6+KTDMP+BJwlrvvNrPJcdWTNWP8GDbtORB3NyIio1KcWwqnA2vcfa27dwN3Apf2m+ZjwA3uvhug393dYjFj/Bg2KxRERAYUZyjMADZEXm8M26KOBY41sz+Y2WNmtnCgBZnZlWbWYmYtra2tr6mo6ePHsGn3Af1Xs4jIAMp9oDkNzAPOBxYDPzSz8f0ncveb3H2Buy9oamp6TR3OmDCGAz297O7QJbRFRPqLMxQ2AbMir2eGbVEbgSXu3uPufwReJAiJ2MwYXwOgXUgiIgOIMxSWAfPMbI6ZVQGLgCX9pvklwVYCZtZIsDtpbYw1MWP8WAA27u6IsxsRkVEptlBw9wxwFXA/8Dxwt7uvNLOvmdkl4WT3AzvNbBXwIPB5d98ZV00ARzUGobB2R3uc3YiIjEqxnZIK4O73Aff1a7s2MuzAZ8PHsBhXU8mUcdWs2b5/uLoUERk1yn2guSyOmVzHy63aUhAR6S+RoXB0Ux0vb9+v01JFRPpJZCgcM7mO/V0Ztuu+CiIih0hsKAC8sHVfmSsRERlZEhkKr5/eAMBzm9rKXImIyMiSyFBoGFPJnMZaVmzcU+5SRERGlESGAsAJMxp4dqO2FEREohIbCifObGBzWyetOtgsIpKT2FA4eVZw3b3l63eXuRIRkZEjsaFw4szxjKlM8djaWK+qISIyqiQ2FKrSFZw2ZyKPvryj3KWIiIwYiQ0FgDcdPYkXt+1n+z7ds1lEBBIeCmcd3QjAwy9qa0FEBBIeCm+YMY5pDTXcv3JruUsRERkREh0KZsafvX4qD7/YSntXptzliIiUXaJDAWDhG6bSlenjwdXby12KiEjZJT4UTmueyJRx1fziyf63jxYRSZ7Eh0KqwviLP5nFQ6u3s6XtQLnLEREpq8SHAsC7F8yiz+Gelo3lLkVEpKwUCsDsSWM5+5hGbnt8PV2Z3nKXIyJSNrGGgpktNLPVZrbGzK4uMN27zMzNbEGc9RTy8fPmsm1vl44tiEiixRYKZpYCbgAuBOYDi81s/gDT1QOfBh6Pq5ZSnH1MIyfMaODG371MprevnKWIiJRNnFsKpwNr3H2tu3cDdwKXDjDd3wFfB8p6rQkz46q3HMP6nR3c1bKhnKWIiJRNnKEwA4h+u24M23LM7FRglrv/V6EFmdmVZtZiZi2tra2Hv9LQBfOncHrzRL619EX2dfbE1o+IyEhVtgPNZlYBfAv4XLFp3f0md1/g7guamprirIkvX/w6drZ3853/fim2fkRERqo4Q2ETMCvyembYllUPvAF4yMzWAWcAS8p5sBmC+ywsPn0WP/79H3nqFd2AR0SSJc5QWAbMM7M5ZlYFLAKWZEe6e5u7N7p7s7s3A48Bl7h7S4w1leRvL3odU8fV8Df3PENnj05RFZHkiC0U3D0DXAXcDzwP3O3uK83sa2Z2SVz9Hg71NZVc/64Tebm1neuWrCx3OSIiwyYd58Ld/T7gvn5t1+aZ9vw4axmsc49t4qo3H8P3HlzDybPGs+j02eUuSUQkdvqP5gL++m3Hcs68Rq79j5U8ukY34hGRI59CoYBUhfHdxafQ3DiWK29dznOb2spdkohIrBQKRYwfW8VPLn8jDWMq+dDNT7Bq895ylyQiEhuFQgmmNtRw6xWnU5WuYNFN/8Py9TpVVUSOTAqFEs1tquOeT5zJhNoqPvDjx1mq+zqLyBFIoTAIMyeM5Z6Pn8m8yXVceetyvvvfL+Hu5S5LROSwUSgM0uRxNdz18TN5xykz+OZvXuRjP1nOzv1d5S5LROSwUCgMQU1lim+9+ySuvXg+D7/YysJvP8JDq7eXuywRkddMoTBEZsblZ8/hl395FhPGVvLhf1/G5+95hl3t3eUuTURkyBQKr9H86eNYctXZfOK8o7n3qU285ZsPcfeyDfT16ViDiIw+CoXDoKYyxdUXHs9//dU5zJtcxxd+voKLv/t7Hlq9XQeiRWRUUSgcRsdNreeuK8/k24tOZl9XDx/+92W894ePs3z9rnKXJiJSEhttf8kuWLDAW1rKfnXtorozfdz++Hq++8AadrZ3c3rzRD55/tGcf1wTZlbu8kQkYcxsubsXvV+NQiFmHd0Z7nxiAz96ZC2b2zo5fmo9HzmrmT8/aTpjq2K9SK2ISI5CYYTpzvSx5JnN/PDhtazeto/6mjTvOnUm73vjbOZNqS93eSJyhFMojFDuTsv63fz0sfX86tmtdPf2ccrs8bz95BlcfOI0JtVVl7tEETkCKRRGgZ37u/jZ8o3c+9QmXti6j1SFce68Rt5+ygzecvxk6msqy12iiBwhFAqjzAtb9/LLpzaz5OlNbG7rpDJlnHl0IxfMn8Lb5k9hyriacpcoIqOYQmGU6utzlr+ym9+s2sbSlVtZt7MDgJNmjef8Y5s499hGTpo5nnRKZxOLSOkUCkcAd2fN9v0sXbWNpau2sWLjHtyhvjrNGUdP4tx5jZw9r4nmSWN1mquIFDQiQsHMFgLfBlLAj9z9+n7jPwt8FMgArcDl7r6+0DKTFAr97eno5tGXd/LIS6088tIONu4+AMDk+mpOa57IguYJnNY8keOn1mtLQkQOUfZQMLMU8CLwNmAjsAxY7O6rItO8GXjc3TvM7JPA+e7+nkLLTXIoRLk763d28Ps1O2hZt4tl63azaU8QErVVKU49agKnzp7ASbMaOGHGeJrqdVaTSJKVGgpx/vfU6cAad18bFnQncCmQCwV3fzAy/WPA+2Os54hiZjQ31tLcWMv7zzgKgM17DtCyfjct63bxxB938Z0HXiKb+dMaajhhRgMnzmzgxJnjOWFGAxNqq8r4DkRkJIozFGYAGyKvNwJvLDD9FcCvBhphZlcCVwLMnj37cNV3xJk+fgyXjB/DJSdNB6C9K8OqLXt5ZsMent3UxrMb21i6altu+injqjlu6jiOn1rPcVPqOW5qPcdMrqOmMlWutyAiZTYirrNgZu8HFgDnDTTe3W8CboJg99Ewljaq1VanOa15Iqc1T8y17e3s4bkwIFZv3cfqbfu45dGddGf6AKgwaG6s5fip9RzTVMfcpjrmNNYyt6lW/zchkgBxhsImYFbk9cyw7RBm9lbgGuA8d9d9LWM2rqaSNx3dyJuObsy1ZXr7WLezIwiJrXt5Yes+Vm7ey6+f20r0thCNddXMbaplbhgScxrrmNM4lpkTxmrrQuQIEWcoLAPmmdkcgjBYBLw3OoGZnQL8AFjo7rqfZZmkUxUcM7mOYybX8b9OnJZr78r08srODtbuaGdtaztrW/fzxx3tLF217VV3mGuqr2bWhDHMmjiWWRPGMmvimPB5LNMaanQ2lMgoEVsouHvGzK4C7ic4JfVmd19pZl8DWtx9CfANoA64JzzP/hV3vySummRwqtMp5k2pH/CCfXs6ulm7o51XdnawYVcHG3Z3sGHXAZav381/rthCb2QTI1VhTGuoYXrDGKY21DCtoSbyPIZpDTU01lWTqtD/WoiUm/55TQ67nt4+trZ1HhIWG3d3sKWtk617O9nS1pk7hpGVqjCm1FeHYTGGKeNqmDyumsa6aprqq2mqq6axvopJtQoPkaEYCaekSkJVpiqC3UgTxw443t3Z3dHDlrYDbG3rZHNbJ1vbDgSh0dbJ81v28sAL2znQ0/uqeSsMJtZW01hXFYRFGBjZ4Um11YwfW8nE2iom1lbpWIfIICkUZNiZWe5L+/XTG/JO196VYcf+Llr3hY/9XewIn4PX3axtbad1XxfdvX0DLmNMZYqJtVVMqK1kwtigz9xzbRUTxwbjsu3jaiqpqazQZUMksRQKMmLVVqeprU5z1KTagtO5O3s7M7Tu62RXew+72rvZ3dEdPLd3s6sjeN7d0cMruzrY1d7Nvs5M3uVVpSoYN6aScWPSNIypZFxNZfAcvj607dDX9TVpKrR7S0YxhYKMemaW+7IuVXemjz0HutkdCZHdHd20Hehh74FM+NzD3s4ednd0s35ne9DWmTnkIPqra4G6qjR1NUGg1VWnqa8Jngd6nR1+1euaNNVp7fqS4adQkESqSlcwub6GyfWDu0+Fu9Pe3ZsLjbbwsTfyvK8rw/7ODPu7Dj62tnUGw50Z9ndnKOX8jsqU5cKitirNmKoUY6tSjK1Kh88pxlSlDhkXHc6O6z9ddVq7xyQ/hYLIIJhZ7i/7GePHDGkZfX3OgZ5e9ndl2NeZoT0Mjuhw/9cd3Rk6unvp6O5ld8cBDnRnaO/u5UB3Lx3dGQpsvLxKhdEvWNLUVFZQk04Fz5UpxlSmqK48+LomnWJM1cHh6mx7OG10uprKCmqqguHKlCmARhmFgsgwq6iw3PGSKeNe+/Lcna5MXxgaGQ5099IeGc62Z0MlGB8d10tXppfOnl527M9woCcY7uzpo6unl85MLz29Qzt1vcI4JDyqKyuoTgdbK1XpCqrDRzCcoip1sD3Xlu7fNsD4VAXVldnnQ5ejLaPBUSiIjHJmlvvinRjTlW8zvX10ZvrCsAgC41XDmSBwOjNhmETGHcgOZ3rp6umlK9NHd6aP/V0Zdu7vo7u3j65ML91he3Z8ZjCbQAVkQyIaLukKozJsr0xVUJkKX6fC1+mwraKCynS/camgLfs63X/elFGZ7ve63/BA/Y6EkxQUCiJSVDpVQV2qgrrq4f3K6O3zSFCEYdLbR1dP9rmX7t5DgyQbLl2RtoPz9NLV00dPbx89vU53bzCcCYf3d2WCcRmnp7cvN76n1+nJ9NHTFwwXOtngtUhVWC6s0ikjXXEwNNIp472nz+aj58yNpe8shYKIjFipCmNMeKAcRs5Venv7PAyLMDDCYIq+PmRcb18QKpHXmch03ZEgys6XCQMok23r82G5WZZCQURkkFIVRqoidUT+x7wuXSkiIjkKBRERyVEoiIhIjkJBRERyFAoiIpKjUBARkRyFgoiI5CgUREQkZ9Tdo9nMWoH1Q5y9EdhxGMs5XEZqXTBya1Ndg6O6BudIrOsod28qNtGoC4XXwsxaSrlx9XAbqXXByK1NdQ2O6hqcJNel3UciIpKjUBARkZykhcJN5S4gj5FaF4zc2lTX4KiuwUlsXYk6piAiIoUlbUtBREQKUCiIiMhB7p6IB7AQWA2sAa6OYfmzgAeBVcBK4NNh+3XAJuDp8HFRZJ4vhfWsBv6sWK3AHODxsP0uoGoQ9a0Dng1raAnbJgK/AV4KnyeE7QZ8J+xnBXBqZDkfCqd/CfhQpP1PwuWvCee1Emo6LrJengb2Ap8pxzoDbga2A89F2mJfP/n6KFLXN4AXwr7vBcaH7c3Agch6u3Go/Rd6jwXqiv1zA6rD12vC8c0l1HVXpKZ1wNNlWF/5vh/K/jP2qt+Fw/3lOBIfQAp4GZgLVAHPAPMPcx/Tsh8cUA+8CMwPf1H+ZoDp54d1VIe/AC+HdeatFbgbWBQO3wh8chD1rQMa+7X9c/YXEbga+Ho4fBHwq/AH8wzg8cgP19rweUI4nP0hfiKc1sJ5LxzCZ7QVOKoc6ww4FziVQ79MYl8/+fooUtcFQDoc/nqkrubodP2WM6j+873HInXF/rkB/5vwyxtYBNxVrK5+478JXFuG9ZXv+6HsP2Oveu+D/fIbjQ/gTOD+yOsvAV+Kuc//AN5W4BflkBqA+8M6B6w1/KB3cPDL4JDpSqhnHa8OhdXAtMgP7epw+AfA4v7TAYuBH0TafxC2TQNeiLQfMl2J9V0A/CEcLufUkNIAAAXVSURBVMs6o9+XxHCsn3x9FKqr37h3ALcVmm4o/ed7j0XWV+yfW3becDgdTmeF6oq0G7ABmFeO9dWvj+z3w4j4GYs+knJMYQbBD0PWxrAtFmbWDJxCsHkLcJWZrTCzm81sQpGa8rVPAva4e6Zfe6kcWGpmy83syrBtirtvCYe3AlOGWNuMcLh/+2AsAu6IvB4J62w41k++Pkp1OcFfhVlzzOwpM/udmZ0TqXew/Q/1dybuzy03Tzi+LZy+FOcA29z9pUjbsK+vft8PI+5nLCmhMGzMrA74OfAZd98LfB84GjgZ2EKw+VoOZ7v7qcCFwF+a2bnRkR78GeHlKMzMqoBLgHvCppGyznKGY/0Mtg8zuwbIALeFTVuA2e5+CvBZ4HYzGxdX/wMYcZ9bP4s59A+PYV9fA3w/vKblDVYpfSQlFDYRHOjJmhm2HVZmVknwgd/m7r8AcPdt7t7r7n3AD4HTi9SUr30nMN7M0kN5D+6+KXzeTnBw8nRgm5lNC2ufRnCAbii1bQqH+7eX6kLgSXffFtY4ItYZw7N+8vVRkJl9GLgYeF/4i467d7n7znB4OcH++mOH2P+gf2eG6XPLzROObwinLyic9p0EB52z9Q7r+hro+2EIy4v9ZywpobAMmGdmc8K/ShcBSw5nB2ZmwI+B5939W5H2aZHJ3gE8Fw4vARaZWbWZzQHmERwoGrDW8Bf/QeCycP4PEeyXLKW2WjOrzw4T7L9/LqzhQwMsbwnwQQucAbSFm5/3AxeY2YRw18AFBPt6twB7zeyMcD18sNTaQof8BTcS1lmkv7jXT74+8jKzhcAXgEvcvSPS3mRmqXB4brh+1g6x/3zvsVBdw/G5Reu9DHggG4pFvJVgn3tuF8twrq983w9DWF78P2OFDjgcSQ+Co/kvEvw1cE0Myz+bYLNsBZFT8oBbCU4TWxF+ONMi81wT1rOayNk6+WolOEvjCYJTzu4BqkusbS7BmR3PEJwOd03YPgn4b4JT1X4LTAzbDbgh7P9ZYEFkWZeH/a8BPhJpX0DwJfAy8D1KOCU1nK+W4C+9hkjbsK8zglDaAvQQ7I+9YjjWT74+itS1hmC/8iGnUgLvCj/fp4EngT8fav+F3mOBumL/3ICa8PWacPzcYnWF7bcAn+g37XCur3zfD2X/Gev/0GUuREQkJym7j0REpAQKBRERyVEoiIhIjkJBRERyFAoiIpKjUJDEMbNHw+dmM3vvYV723w7Ul8hooVNSJbHM7HyCC7hdPIh50n7wmjwDjd/v7nWHoz6RctCWgiSOme0PB68HzjGzp83sr80sZWbfMLNlFlzU7ePh9Oeb2SNmtoTgeviY2S8tuLjgSgsvMGhm1wNjwuXdFu0r/M/Ub5jZc2b2rJm9J7Lsh8zsZ2b2gpndFv5HKmZ2vZmtCmv5v8O5jiS50sUnETliXU1kSyH8cm9z99PMrBr4g5ktDac9FXiDu/8xfH25u+8yszHAMjP7ubtfbWZXufvJA/T1ToILxZ0ENIbzPByOOwV4PbAZ+ANwlpk9T3CpiOPd3c1s/GF/9yID0JaCyEEXEFxv5mmCyxpPIrgeDsATkUAA+CszewZ4jOACZfMo7GzgDg8uGLcN+B1wWmTZGz24kNzTBNf5bwM6gR+b2TuBjgGWKXLYKRREDjLgU+5+cviY4+7ZLYX23ETBsYi3Etzs5STgKYJr8gxVV2S4l+DmMhmCq4z+jOBqqL9+DcsXKZlCQZJsH8GtEbPuBz5pwSWOMbNjLbiqbH8NwG537zCz4wlugZjVk52/n0eA94THLZoIbhv5RL7CLLjufoO73wf8NcFuJ5HY6ZiCJNkKoDfcDXQL8G2CXTdPhgd7W4G3DzDfr4FPhPv9VxPsQsq6CVhhZk+6+/si7fcS3FbyGYKrZX7B3beGoTKQeuA/zKyGYAvms0N7iyKDo1NSRUQkR7uPREQkR6EgIiI5CgUREclRKIiISI5CQUREchQKIiKSo1AQEZGc/w8uNnahts0yFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR5OPChWS6Jq",
        "outputId": "fe9fb8fc-08d6-46e1-ff92-af7cb8b1de57"
      },
      "source": [
        "print(\">> Original label: \\n\", y_test)\n",
        "y_pred = lr_bgd.predict(X_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred)\n",
        "print(\"\\n>> Score: \", lr_bgd.score(X_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n",
            "\n",
            ">> Predicted label: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 1 2 1 2]\n",
            "\n",
            ">> Score:  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QBi9Wwap7Fy"
      },
      "source": [
        "**Kết quả bài toán Logistic Regression với bộ dữ liệu *Iris* chạy bằng thư viện scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCVEX-XxaqKg",
        "outputId": "12c86e71-471f-464e-fe94-05e3884ee571"
      },
      "source": [
        "lr = LogisticRegression(random_state=10)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "score = lr.score(X_test, y_test)\n",
        "\n",
        "print(\">> Original label: \\n\", y_test)\n",
        "print(\"\\n>> Predicted label: \\n\", y_pred)\n",
        "print(\"\\n>> Score: \", score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Original label: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n",
            "\n",
            ">> Predicted label: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 2\n",
            " 0 0 0 2 2 2 0 0 1 1 2 1 2]\n",
            "\n",
            ">> Score:  0.9\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}