{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.spatial.distance import cdist # Tính khoảng cách giữa các cặp điểm trong 2 tập hợp một cách hiệu quả\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "rs = 10\n",
    "rnd = np.random.RandomState(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo bộ dữ liệu ngẫu nhiên\n",
    "Ta tạo một bộ dữ liệu ngẫu nhiên gồm 3 cụm có tâm lần lượt là `(2, 2)`, `(8, 3)` và `(3, 6)` với 200 mẫu cho mỗi cụm theo phân phối chuẩn.\n",
    "\n",
    "### Tạo bộ dữ liệu bằng thư viện numpy\n",
    "Dữ liệu được tạo ra bằng cách lấy ngẫu nhiên 200 điểm cho mỗi cụm theo phân phối chuẩn có kỳ vọng lần lượt là `(2, 2)`, `(8, 3)` và `(3, 6)`; ma trận hiệp phương sai giống nhau và là ma trận đơn vị cấp 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(18) # Cố định dữ liệu random sau mỗi lần chạy là giống nhau\n",
    "# Danh sách các tâm cụm đồng thời là kỳ vọng để tạo mẫu\n",
    "means = [[2, 2], [8, 3], [3, 6]]\n",
    "# Ma trận hiệp phương sai cấp 2\n",
    "cov = [[1, 0], [0, 1]]\n",
    "# Số lượng điểm mẫu cho mỗi cụm\n",
    "N = 200\n",
    "# Tạo mẫu ngẫu nhiên 200 điểm cho mỗi cụm theo phân phối chuẩn có kỳ vọng (means) và ma trận hiệp phương sai (cov)\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "# Nối X0, X1, X2 thành một bộ dữ liệu lớn\n",
    "X_np = np.concatenate((X0, X1, X2), axis = 0)\n",
    "K = 3 # 3 cụm\n",
    "# Tạo vector cột làm nhãn cho dữ liệu ở trên\n",
    "y_np = np.asarray([0]*N + [1]*N + [2]*N).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô phỏng dữ liệu vừa tạo bằng đồ thị."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_np\n",
    "y = y_np\n",
    "labels = np.unique(y)\n",
    "marker = ['o', 'v', 's']\n",
    "#Minh hoạ kết quả\n",
    "fig, ax = plt.subplots()\n",
    "for i, m in zip(labels, marker):\n",
    "    ax.scatter(X[y == i][:, 0], X[y == i][:, 1], edgecolor='k', marker=m)\n",
    "    \n",
    "plt.legend(labels)\n",
    "plt.title('Simple dataset with 3 clusters using numpy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tạo bộ dữ liệu bằng thư viện scikit-learn\n",
    "Ta dùm hàm `sklean.datasets.make_blobs` để tạo ra một bộ dữ liệu ngẫu nhiên có 3 cụm và 200 điểm dữ liệu cho mỗi cụm theo phân phối chuẩn với các tâm cụm cho trước như trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo hàm make_blobs từ thư viện sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "# Hàm make_blobs tạo bộ dữ liệu ngẫu nhiên trả về X là danh sách các mẫu và y là danh sách nhãn của các mẫu\n",
    "X_skl, y_skl = make_blobs(n_samples=600, \n",
    "                  n_features=2, \n",
    "                  centers=[[2, 2], [8, 3], [3, 6]], \n",
    "                  cluster_std=1, \n",
    "                  shuffle=True, \n",
    "                  random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_skl\n",
    "y = y_skl\n",
    "labels = np.unique(y)\n",
    "marker = ['o', 'v', 's']\n",
    "#Minh hoạ kết quả\n",
    "fig, ax = plt.subplots()\n",
    "for i, m in zip(labels, marker):\n",
    "    ax.scatter(X[y == i][:, 0], X[y == i][:, 1], edgecolor='k', marker=m)\n",
    "\n",
    "plt.legend(labels)\n",
    "plt.title('Simple dataset with 3 clusters using scikit-learn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thuật toán K-Means\n",
    "## Nhắc lại các bước của thuật toán K-Means\n",
    "1. Chọn **K** điểm bất kỳ trong tập huấn luyện làm các tâm cụmban đầu.\n",
    "2. Phân mỗi điểm dữ liệu vào cụm có tâm gần nhất.\n",
    "3. Cập nhật lại tâm cụm bằng cách lấy trung bình cộng củacác điểm đã được gán vào cụm đó sau bước 2.\n",
    "4. Nếu tâm cụm mới được hình thành không thay đổi so với vòng lặp trước thì dừng lại.\n",
    "5. Quay lại bước 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viết các hàm:\n",
    "1. `kmeans_init_centroids` để khởi tạo các điểm đại diện ban đầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khởi tạo tâm cụm\n",
    "def kmeans_init_centroids(X, n_cluster):\n",
    "    '''\n",
    "    - Khởi tạo các điểm đại diện (centroid) ban đầu\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào\n",
    "        \n",
    "    n_cluster : int\n",
    "        Số centroid\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    Danh sách k điểm ngẫu nhiên trong X làm centroids\n",
    "    '''\n",
    "    # Chọn ngẫu nhiên k dòng của X để tạo tâm cụm\n",
    "    if isinstance(X, pd.DataFrame): # kiểm tra X có phải là 1 DataFrame hay không\n",
    "        return X.loc[rnd.choice(X.shape[0], n_cluster, replace=False)]\n",
    "    return X[rnd.choice(X.shape[0], n_cluster, replace=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `kmeans_asign_labels` để gán nhán mới cho các điểm tương ứng với các điểm đại diện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm nhãn mới cho các điểm khi biết tâm cụm\n",
    "def kmeans_assign_labels(X, centroids):\n",
    "    '''\n",
    "    - Gán nhãn mới cho các điểm dữ liệu khi biết centroids\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào\n",
    "        \n",
    "    centroids : numpy.ndarray\n",
    "        Danh sách các điểm tâm cụm (centroid)\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    Nhãn mới của từng mẫu trong X ứng với tâm cụm gần nhất.\n",
    "    '''\n",
    "    #Tính khoảng cách giữa X và tâm cụm\n",
    "    D = cdist(X, centroids)\n",
    "    #Trả về Tâm cụm gần nhất\n",
    "    return np.argmin(D, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `kmeans_update_centroids` để cập nhật các điểm đại diện mới dựa trên dữ liệu vừa được gán nhãn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kiểm tra tính hội tụ, điều kiện dừng của bài toán\n",
    "def has_converged(centroids, new_centroids):\n",
    "    '''\n",
    "    - Kiểm tra tính hội tụ và điều kiện dừng của bài toán.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    centroids : numpy.ndarray\n",
    "        Danh sách các điểm tâm cụm ban đầu.\n",
    "        \n",
    "    new_centroids : numpy.ndarray\n",
    "        Danh sách các điểm tâm cụm mới.\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    True nếu 2 tâm cụm là như nhau, ngược lại là False.\n",
    "    '''\n",
    "    #Trả về True nếu tập hợp 2 tâm cụm là như nhau\n",
    "    return (set([tuple(a) for a in centroids]) == set([tuple(a) for a in new_centroids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `has_converged` để kiểm tra điều kiện dừng của thuật toán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cập nhật Tâm cụm khi biết nhãn của từng điểm\n",
    "def kmeans_update_centroids(X, labels, K):\n",
    "    '''\n",
    "    - Cập nhật lại tâm cụm khi biết nhãn của từng dữ liệu.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào.\n",
    "    labels : numpy.ndarray\n",
    "        Danh sách nhãn của từng mẫu trong dữ liệu.\n",
    "    K : int\n",
    "        Số tâm cụm.\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    centroids : numppy.ndarray\n",
    "        Danh sách các tâm cụm mới.\n",
    "    '''\n",
    "    centroids = np.zeros((K, X.shape[1]))\n",
    "    for k in range(K):\n",
    "        #Tập hợp tất cả các điểm mà ứng với tâm cụm thứ k\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            Xk = X.loc[labels == k, :]\n",
    "        else:\n",
    "            Xk = X[labels == k, :]\n",
    "        #Tính trung bình\n",
    "        centroids[k,:] = np.mean(Xk, axis = 0)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần chính của thuật toán K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, n_clusters):\n",
    "    '''\n",
    "    - Thực thi thuật toán K-Means\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào.\n",
    "        \n",
    "    n_clusters : int\n",
    "        Số tâm cụm (số cụm).\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    centroids : numpy.ndarray\n",
    "        Danh sách các tâm cụm sau mỗi lần thực thi thuật toán K-Means.\n",
    "        \n",
    "    labels : numpy.ndarray\n",
    "        Danh sách nhãn của từng mẫu trong dữ liệu ban đầu sau mỗi lần thực thi thuật toán.\n",
    "        \n",
    "    times : int\n",
    "        Số lần lặp lại bước 2 và 3 để cho ra kết quả cuối cùng.\n",
    "    '''\n",
    "    centroids = [kmeans_init_centroids(X, n_clusters)]\n",
    "    labels = []\n",
    "    times = 0 \n",
    "    while True:\n",
    "        labels.append(kmeans_assign_labels(X, centroids[-1]))\n",
    "        new_centroids = kmeans_update_centroids(X, labels[-1], n_clusters)\n",
    "        if has_converged(centroids[-1], new_centroids):\n",
    "            break\n",
    "        centroids.append(new_centroids)\n",
    "        times += 1\n",
    "    return (centroids, labels, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `SSE` tính tổng bình phương sai số của toàn bộ dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính tổng bình phương sai số SSE\n",
    "def SSE(X, centroids):\n",
    "    '''\n",
    "    - Tính tổng bình phương sai số của toàn bộ dữ liệu X.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào.\n",
    "        \n",
    "    centroids : numpy.ndarray\n",
    "        Các tâm cụm cuối cùng sau khi thực thi thuật toán K-Means.\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    sse : float\n",
    "        Tổng bình phương sai số toàn bộ dữ liệu.\n",
    "    '''\n",
    "    D = cdist(X, centroids, 'euclidean')\n",
    "    dist = np.min(D, axis=1)\n",
    "    sse = sum(dist**2)\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(centroids, labels, times) = kmeans(X_np, K)\n",
    "print('Centers found by our algorithm:\\n', centroids[-1])\n",
    "X = X_np\n",
    "y = labels[-1]\n",
    "centers = centroids[-1]\n",
    "label = np.unique(y)\n",
    "marker = ['o', 'v', 's']\n",
    "plt_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'w'] # danh sách các màu hỗ trợ\n",
    "#Minh hoạ kết quả\n",
    "fig, ax = plt.subplots()\n",
    "for i, m in zip(label, marker):\n",
    "    ax.scatter(X[y == i][:, 0], X[y == i][:, 1], edgecolor='k', marker=m, label = 'cluster_' + str(i))\n",
    "    ax.plot(centers[i][0], centers[i][1], plt_colors[i+2] + 'o', markersize = 10, label = 'center_' + str(i)) # Vẽ tâm cụm i lên đồ thị\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), ncol=2, borderaxespad=0.)\n",
    "plt.title('Result using our K-Means algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô tả từng bước thực hiện thuật toán K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = range(times)\n",
    "for time in iters:\n",
    "    filename = 'Assigned label for data at time = ' + str(time + 1)\n",
    "    X = X_np\n",
    "    y = labels[time]\n",
    "    centers = centroids[time]\n",
    "    label = np.unique(y)\n",
    "    marker = ['o', 'v', 's']\n",
    "    plt_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'w'] # danh sách các màu hỗ trợ\n",
    "    #Minh hoạ kết quả\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, m in zip(label, marker):\n",
    "        ax.scatter(X[y == i][:, 0], X[y == i][:, 1], edgecolor='k', marker=m, label = 'cluster_' + str(i))\n",
    "        ax.plot(centers[i][0], centers[i][1], plt_colors[i+2] + 'o', markersize = 10, label = 'center_' + str(i)) # Vẽ tâm cụm i lên đồ thị\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), ncol=2)\n",
    "    plt.title(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ kết quả này chúng ta thấy rằng thuật toán K-means Clustering làm việc khá thành công, các centroids tìm được khá gần với kỳ vọng ban đầu. Mặc dù màu sắc bị hoán đổi nhưng những điểm ban đầu thuộc cùng một cluster gần như vẫn cùng thuộc một cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết quả tìm được bằng thư viện scikit-learn\n",
    "Một số thông số quan trọng trong hàm `sklearn.cluster.KMeans()`:\n",
    "* `n_clusters` : `int`, mặc định = 8\n",
    "    Số lượng cụm hình thành (số lượng centroids cầntạo).\n",
    "* `init` : `{'k-means++', 'random'}`, mặc định là `'k-means++`\n",
    "    Phương thức khởi tạo.\n",
    "* `n_init` : `int`, mặc định = 10\n",
    "    Số lần thuật toán K-Means chạy với bộ tâm khác nhau.\n",
    "* `max_iter` : `int`, mặc định = 300\n",
    "    Số lần lặp tối đa cho một lần chạy thuật toán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # Khai báo thư viện K-Means\n",
    "# Khởi tạo thuật toán K-Means với 3 cụm và các tâm cụm được tạo ngẫu nhiên\n",
    "model = KMeans(n_clusters=3, init='random', random_state=rs)\n",
    "# Áp dụng thuật toán K-Means lên dữ liệu X_np\n",
    "model.fit(X_np)\n",
    "print('Centers found by scikit-learn:')\n",
    "# model.cluster_centers_ là hàm lấy tâm cụm sau khi thực hiện thuật toán K-Means\n",
    "print(model.cluster_centers_)\n",
    "# Dùng hàm model.predict để lấy ra kết quả phân cụm dữ liệu ban đầu sau khi thực hiện thuật toán K-Means\n",
    "pred_label = model.predict(X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minh hoạ kết quả\n",
    "X = X_np\n",
    "y = pred_label\n",
    "centers = model.cluster_centers_\n",
    "labels = np.unique(y)\n",
    "marker = ['o', 'v', 's']\n",
    "plt_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'w'] # danh sách các màu hỗ trợ\n",
    "fig, ax = plt.subplots()\n",
    "for i, m in zip(labels, marker):\n",
    "    ax.scatter(X[y == i][:, 0], X[y == i][:, 1], edgecolor='k', marker=m, label = 'cluster_' + str(i))\n",
    "    ax.plot(centers[i][0], centers[i][1], plt_colors[i+2] + 'o', markersize = 10, label = 'center_' + str(i)) # Vẽ tâm cụm i lên đồ thị\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), ncol=2, borderaxespad=0.)\n",
    "plt.title('Result using scikit-learn K-Means algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta thấy rằng hai thuật toán khác nhau cho cùng một đáp số."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thực thi thuật toán K-Means trên bộ dữ liệu thực\n",
    "Trong phần này, ta sẽ sử dụng bộ dữ liệu `iris` từ thư viện `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#Load dữ liệu\n",
    "data, original_label = load_iris(return_X_y=True, as_frame=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Shape of data: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phương pháp elbow giúp tìm số cụm tối ưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    (centroids, labels, times) = kmeans(data, i)\n",
    "    sse = SSE(data, centroids[-1])\n",
    "    distortions.append(sse)\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo phương thức elbow thì tại `k = 3` (tức số cụm là 3) sẽ cho ra kết quả của thuật toán K-Means tốt nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phương pháp xác định cụm tối ưu bằng silhouette plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giảm số chiều của dữ liệu về 2 chiều để trực quan hoá trên biểu đồ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Chuẩn hóa dữ liệu\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(data)\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trực quan hoá biểu đồ `silhouette` để chọn ra số cụm tối ưu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Khai báo hàm thư viện của silhouette\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "for n_clusters in range(2,6):\n",
    "    # Khởi tạo subplot gồm 1 dòng, 2 cột \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(12, 4)\n",
    "\n",
    "    # Ô đầu tiên trong subplot là silhouette plot\n",
    "    # Hệ số silhoutte có thể thuộc [-1,1] nhưng trong ví dụ này hệ số thuộc [-0.1,1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    \n",
    "    # (N_clusters + 1) * 10: để chèn khoảng trống giữa các silhouette plots để phân cách chúng một cách rõ ràng\n",
    "    ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    # Khởi tạo clusters với giá trị n_clusters \n",
    "    (centroids, labels, times) = kmeans(data, n_clusters)\n",
    "    y_predict = labels[-1]\n",
    "    \n",
    "    # Silhouette_score cho giá trị trung bình của tất cả các mẫu.\n",
    "    # Điều này cho ta thấy về mật độ và sự tách biệt của các cụm\n",
    "    silhouette_avg = silhouette_score(data, y_predict)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Tính silhouette scores cho mỗi mẫu\n",
    "    sample_silhouette_values = silhouette_samples(data, y_predict)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Tổng hợp silhouette scores cho các mẫu thuộc về cụm i và sắp xếp chúng\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[y_predict == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "     \n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Tính y_lower cho plot tiếp theo\n",
    "        y_lower = y_upper + 10  \n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # Đường thẳng nét đứt, màu đỏ là silhouette scores trung bình của tất cả các giá trị\n",
    "\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Xóa yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    # Ô thứ 2 trong subplot hiển thị các cụm thực tế được hình thành\n",
    "    colors = cm.nipy_spectral(y_predict.astype(float) / n_clusters)\n",
    "    ax2.scatter(X_pca[:,0],\n",
    "    X_pca[:,1] , c = colors)\n",
    "    \n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC 1\")\n",
    "    ax2.set_ylabel(\"PC 2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo phương thức silhouette thì tại `k = 3` (tức số cụm là 3) sẽ cho ra kết quả của thuật toán K-Means tốt nhất.\n",
    "\n",
    "Như vậy, đối chiếu kết quả thì cả 2 phương pháp đều cho ra cùng đáp số là `k = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thực thi thuật toán K-Means bằng hàm tự xây dựng với số cụm vừa tìm được ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(centroids, labels, times) = kmeans(data, 3)\n",
    "print('Centers found by our algorithm:\\n', centroids[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thực thi thuật toán K-Means bằng hàm từ thư viện scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo thuật toán K-Means với 3 cụm và các tâm cụm được tạo ngẫu nhiên\n",
    "model = KMeans(n_clusters=3, init='random', random_state=rs)\n",
    "# Áp dụng thuật toán K-Means lên dữ liệu data\n",
    "model.fit(data)\n",
    "print('Centers found by scikit-learn:')\n",
    "# model.cluster_centers_ là hàm lấy tâm cụm sau khi thực hiện thuật toán K-Means\n",
    "print(model.cluster_centers_)\n",
    "# Dùng hàm model.predict để lấy ra kết quả phân cụm dữ liệu ban đầu sau khi thực hiện thuật toán K-Means\n",
    "pred_label = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ kết quả này chúng ta thấy rằng *thuật toán K-means Clustering* cho ra kết quả gần giống với kết quả dùng hàm từ thư viện scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\">> Original labels :\")\n",
    "label = np.array(original_label)\n",
    "print(label)\n",
    "print(\">> Predicted labels using our algorithm :\")\n",
    "print(labels[-1])\n",
    "print(\">> Predicted labels found by scikit-learn :\")\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "colors = cm.nipy_spectral(labels[-1].astype(float) / n_clusters)\n",
    "ax1.scatter(X_pca[:,0], X_pca[:,1] , c = colors)\n",
    "\n",
    "ax1.set_title(\"Result of our algorithm\")\n",
    "ax1.set_xlabel(\"PC 1\")\n",
    "ax1.set_ylabel(\"PC 2\")\n",
    "\n",
    "colors = cm.nipy_spectral(pred_label.astype(float) / n_clusters)\n",
    "ax2.scatter(X_pca[:,0], X_pca[:,1] , c = colors)\n",
    "\n",
    "ax2.set_title(\"Result found by scikit-learn\")\n",
    "ax2.set_xlabel(\"PC 1\")\n",
    "ax2.set_ylabel(\"PC 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy rằng mặc dù ký hiệu nhóm không tương ứng giữa kết quả sau khi chạy *thuật toán K-Means* bằng cả 2 cách, nhưng các mẫu tương ứng thuộc cùng một cụm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thuật toán K-Means++\n",
    "## Nhắc lại các bước của thuật toán\n",
    "\n",
    "1.  Khởi tạo một tập rỗng M để lưu trữ K tâm cụm.\n",
    "2. Chọn ngẫu nhiên tâm cụm đầu tiền μ 1 từ các mẫu đầu vào và thêm nó vào M.\n",
    "3. Đối với mỗi điễm dữ liệu x i không có trong M, tìm bình phương khoảng cách d(x i , M ) 2 đến bất kỳ tâm nào trong M.\n",
    "4. Chọn ngẫu nhiên tâm cụm tiếp theo μ p bằng cách sử dụng phân phối xác suất có trọng số bằng $\\frac{d(\\mu, M)^2}{\\sum_id(x_i, M)^2}$\n",
    "5. Lặp lại bước 2 và 3 đến khi tìm đủ K tâm cụm.\n",
    "6. Bây giờ các tâm cụm ban đầu đã được chọn, ta tiếp tục thực hiện các bước tiếp theo tương tựthuật toán K-Means cổ điển."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(dist):\n",
    "    \"\"\" \n",
    "    - Tính toán tổng bình phương chi phí (khoảng cách) từ dữ liệu đến các tâm hiện tại.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    dist : np.ndarray\n",
    "        Ma trận bình phương khoảng cách giữa dữ liệu và các tâm cụm hiện tại.\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    Tổng bình phương chi phí (khoảng cách) từ dữ liệu đến các tâm cụm hiện tại (hằng số chuẩn hoá).\n",
    "    \"\"\"\n",
    "    return np.sum(np.min(dist,axis=1))\n",
    "\n",
    "def distribution(dist,cost):\n",
    "    \"\"\" \n",
    "    - Tính toán phân bố để tạo ra các tâm cụm mới.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    dist : np.ndarray\n",
    "        Ma trận bình phương khoảng cách giữa dữ liệu và các tâm cụm hiện tại.\n",
    "    cost : np.ndarray\n",
    "        Tổng bình phương khoảng cách từ dữ liệu đến các tâm cụm hiện tại\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    Danh sách các trọng số của phân phối.\n",
    "    \"\"\"\n",
    "    return np.min(dist, axis=1)/cost\n",
    "\n",
    "def sample_new(data,distribution,l):\n",
    "    \"\"\" \n",
    "    - Chọn tâm cụm mới dựa trên trọng số của phân phối.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    data : np.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào.\n",
    "    distribution : np.ndarray\n",
    "        Vector trọng số của các tâm cụm mới theo phân phối.\n",
    "    l : int\n",
    "        Số lượng tâm cụm mới cần lấy từ mẫu.\n",
    "    - Trả về: \n",
    "    ------------------\n",
    "    Các tâm cụm mới.                       \n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame): # kiểm tra X có phải là 1 DataFrame hay không\n",
    "        return data.loc[rnd.choice(range(len(distribution)),l,p=distribution),:]\n",
    "    return data[rnd.choice(range(len(distribution)),l,p=distribution),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeansPlusPlus(data, k):    \n",
    "    \"\"\"\n",
    "    - Áp dụng kỹ thuật của thuật toán phân cụm K-Means++ để lấy các tâm cụm khởi tạo.\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    data : np.ndarray/DataFrame\n",
    "        Dữ liệu đầu vào.\n",
    "    \n",
    "    k : int\n",
    "        Số lượng tâm cụm cần tìm.\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    centroids : np.ndarray\n",
    "        Các tâm cụm khởi tạo thực hiện bởi thuật toán K-Means++\n",
    "    \"\"\"\n",
    "    # Lấy ra tâm cụm đầu tiên\n",
    "    if isinstance(data, pd.DataFrame): # kiểm tra X có phải là 1 DataFrame hay không\n",
    "        centroids = data.loc[rnd.choice(data.shape[0],1),:]\n",
    "    else:\n",
    "        centroids = data[rnd.choice(data.shape[0],1),:]\n",
    "    while centroids.shape[0] < k :  \n",
    "        # Lấy bình phương khoảng cách giữa dữ liệu và các tâm cụm hiện tại\n",
    "        dist = cdist(data, centroids)**2\n",
    "        # Tính tổng bình phương khoảng cách từ dữ liệu đến các tâm cụm hiện tại\n",
    "        norm_const = cost(dist)\n",
    "        # Tính phân phối cho từng tâm cụm mới để lấy mẫu\n",
    "        p = distribution(dist,norm_const)\n",
    "        # Lấy ra một tâm cụm mới theo phân phối vừa tính và thêm vào centroids\n",
    "        centroids = np.r_[centroids, sample_new(data,p,1)]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần chính của thuật toán K-Means++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(X, n_clusters):\n",
    "    '''\n",
    "    - Thực thi thuật toán K-Means++\n",
    "    - Các tham số:\n",
    "    ------------------\n",
    "    X : numpy.ndarray\n",
    "        Dữ liệu đầu vào.\n",
    "        \n",
    "    n_clusters : int\n",
    "        Số tâm cụm (số cụm).\n",
    "    - Trả về:\n",
    "    ------------------\n",
    "    centroids : numpy.ndarray\n",
    "        Danh sách các tâm cụm sau mỗi lần thực thi thuật toán K-Means.\n",
    "        \n",
    "    labels : numpy.ndarray\n",
    "        Danh sách nhãn của từng mẫu trong dữ liệu ban đầu sau mỗi lần thực thi thuật toán.\n",
    "        \n",
    "    times : int\n",
    "        Số lần lặp lại bước 2 và 3 để cho ra kết quả cuối cùng.\n",
    "    '''\n",
    "    centroids = [KMeansPlusPlus(X, n_clusters)] # khởi tạo các tâm cụm ban đầu bằng kỹ thuật của thuật toán K-Means++\n",
    "    labels = []\n",
    "    times = 0 \n",
    "    while True:\n",
    "        labels.append(kmeans_assign_labels(X, centroids[-1]))\n",
    "        new_centroids = kmeans_update_centroids(X, labels[-1], n_clusters)\n",
    "        if has_converged(centroids[-1], new_centroids):\n",
    "            break\n",
    "        centroids.append(new_centroids)\n",
    "        times += 1\n",
    "    return (centroids, labels, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thực thi thuật toán K-Means++ trên bộ dữ liệu thực\n",
    "Trong phần này, ta sẽ sử dụng bộ dữ liệu `iris` từ thư viện `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#Load dữ liệu\n",
    "data, original_label = load_iris(return_X_y=True, as_frame=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Shape of data: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chọn số cụm tối ưu bằng phương pháp elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    (centroids, labels, times) = kmeanspp(data, i)\n",
    "    sse = SSE(data, centroids[-1])\n",
    "    distortions.append(sse)\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of Squared Errors')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chọn số cụm tối ưu bằng silhouette plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giảm số chiều của dữ liệu về 2 chiều để trực quan hoá trên biểu đồ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Chuẩn hóa dữ liệu\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(data)\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trực quan hoá biểu đồ `silhouette` để tìm số cụm tối ưu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "for n_clusters in range(2,6):\n",
    "     # Khởi tạo subplot gồm 1 dòng, 2 cột \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(12, 5)\n",
    "\n",
    "    #Ô đầu tiên trong subplot là silhouette plot\n",
    "    #Hệ số silhoutte có thể thuộc [-1,1] nhưng trong ví dụ này hệ số thuộc [-0.1,1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # (N_clusters + 1) * 10: để chèn khoảng trống giữa các silhouette plots để phân cách chúng một cách rõ ràng\n",
    "    ax1.set_ylim([0, len(data) + (n_clusters + 1) * 10])\n",
    "\n",
    "   # Khởi tạo clusterer với giá trị n_clusters \n",
    "    (centroids, labels, times) = kmeanspp(data, n_clusters)\n",
    "    y_predict = labels[-1]\n",
    "\n",
    "    # Silhouette_score cho giá trị trung bình cho tất cả các mẫu.\n",
    "    # Điều này cho ta thấy về mật độ và sự tách biệt của các cụm\n",
    "    silhouette_avg = silhouette_score(data, y_predict)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "     # Tính silhouette scores cho mỗi mẫu\n",
    "    sample_silhouette_values = silhouette_samples(data, y_predict)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Tổng hợp silhouette scores cho các mẫu thuộc về cụm i và sắp xếp chúng\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[y_predict == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Tính y_lower cho plot tiếp theo\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # Đường thẳng nét đứt, màu đỏ là silhouette scores trung bình của tất cả các giá trị\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  \n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "   # Ô thứ 2 trong subplot hiển thị các cụm thực tế được hình thành\n",
    "    colors = cm.nipy_spectral(y_predict.astype(float) / n_clusters)\n",
    "    ax2.scatter(X_pca[:,0],\n",
    "    X_pca[:,1] , c = colors)\n",
    "    \n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC 1\")\n",
    "    ax2.set_ylabel(\"PC 2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thực thi thuật toán K-Means++ bằng hàm tự viết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(centroids, labels, times) = kmeanspp(data, 3)\n",
    "print('Centers found by our algorithm:\\n', centroids[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thực thi thuật toán K-Means++ bằng thư viện scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo thuật toán K-Means với 3 cụm và các tâm cụm được tạo bằng kỹ thuật K-Means++\n",
    "model = KMeans(n_clusters=3, init='k-means++', random_state=rs)\n",
    "# Áp dụng thuật toán K-Means lên dữ liệu data\n",
    "model.fit(data)\n",
    "print('Centers found by scikit-learn:')\n",
    "# model.cluster_centers_ là hàm lấy tâm cụm sau khi thực hiện thuật toán K-Means++\n",
    "print(model.cluster_centers_)\n",
    "# Dùng hàm model.predict để lấy ra kết quả phân cụm dữ liệu ban đầu sau khi thực hiện thuật toán K-Means++\n",
    "pred_label = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Original labels :\")\n",
    "label = np.array(original_label)\n",
    "print(label)\n",
    "print(\">> Predicted labels using our algorithm :\")\n",
    "print(labels[-1])\n",
    "print(\">> Predicted labels found by scikit-learn :\")\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "colors = cm.nipy_spectral(labels[-1].astype(float) / n_clusters)\n",
    "ax1.scatter(X_pca[:,0], X_pca[:,1] , c = colors)\n",
    "\n",
    "ax1.set_title(\"Result of our algorithm\")\n",
    "ax1.set_xlabel(\"PC 1\")\n",
    "ax1.set_ylabel(\"PC 2\")\n",
    "\n",
    "colors = cm.nipy_spectral(pred_label.astype(float) / n_clusters)\n",
    "ax2.scatter(X_pca[:,0], X_pca[:,1] , c = colors)\n",
    "\n",
    "ax2.set_title(\"Result found by scikit-learn\")\n",
    "ax2.set_xlabel(\"PC 1\")\n",
    "ax2.set_ylabel(\"PC 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
